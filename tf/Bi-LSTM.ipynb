{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\nie\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.630 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 138574, '0': 100192}\n",
      "WARNING:tensorflow:From E:\\python_workSpace\\text_match\\LSTM\\data_prepare.py:55: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "train_texta_embedding: [[ 1  2  3  4  5  6  7  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 9 10 11 12 13 14 15 16 17 18 19 20 21 18  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0]\n",
      " [22 23 24 25 26 27 28 29 30  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0]\n",
      " [31 32 33 34 18 35 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0]\n",
      " [37 38 39 40 41 26 42 43  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0]]\n",
      "train_textb_embedding: [[  5   6   7   8   1 232   3   4   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  9  10  11  12  13  14  15  16  17  18  19  20  21   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 25  26  24 271  27  28  29  30   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 76  31  32  18  35  36   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 37  38 566 483  41  26  42  39   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
      "{'0': 4400, '1': 4402}\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-1-baa99d7088a9>:144: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-1-baa99d7088a9>:152: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-1-baa99d7088a9>:118: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "training 1>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [05:03, 15.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次迭代的损失为：1.1273893;准确率为：0.72770256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:03, 44.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.52      0.59      4399\n",
      "           1       0.61      0.75      0.67      4401\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      8800\n",
      "   macro avg       0.64      0.64      0.63      8800\n",
      "weighted avg       0.64      0.64      0.63      8800\n",
      "\n",
      "验证集：loss 0.749922, acc 0.636591, f1 0.63175\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 2>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [05:08, 14.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2次迭代的损失为：0.5773036;准确率为：0.7865131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:06, 27.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.55      0.62      4399\n",
      "           1       0.63      0.76      0.69      4401\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      8800\n",
      "   macro avg       0.67      0.66      0.66      8800\n",
      "weighted avg       0.67      0.66      0.66      8800\n",
      "\n",
      "验证集：loss 0.725393, acc 0.659091, f1 0.655315\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 3>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [05:08, 16.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3次迭代的损失为：0.5560824;准确率为：0.7990492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:03, 53.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.58      0.64      4399\n",
      "           1       0.64      0.77      0.70      4401\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      8800\n",
      "   macro avg       0.68      0.67      0.67      8800\n",
      "weighted avg       0.68      0.67      0.67      8800\n",
      "\n",
      "验证集：loss 0.707341, acc 0.672273, f1 0.669252\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 4>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [04:43, 16.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第4次迭代的损失为：0.54565;准确率为：0.80577594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:03, 53.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.58      0.64      4399\n",
      "           1       0.65      0.77      0.70      4401\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      8800\n",
      "   macro avg       0.68      0.67      0.67      8800\n",
      "weighted avg       0.68      0.67      0.67      8800\n",
      "\n",
      "验证集：loss 0.708362, acc 0.674318, f1 0.671542\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 5>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [04:46, 17.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第5次迭代的损失为：0.5381238;准确率为：0.81000215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:03, 51.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.60      0.65      4399\n",
      "           1       0.65      0.75      0.70      4401\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      8800\n",
      "   macro avg       0.68      0.68      0.67      8800\n",
      "weighted avg       0.68      0.68      0.67      8800\n",
      "\n",
      "验证集：loss 0.709591, acc 0.675682, f1 0.673819\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 6>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [04:57, 15.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第6次迭代的损失为：0.53050995;准确率为：0.8152168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:03, 51.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.59      0.65      4399\n",
      "           1       0.65      0.77      0.71      4401\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      8800\n",
      "   macro avg       0.69      0.68      0.68      8800\n",
      "weighted avg       0.69      0.68      0.68      8800\n",
      "\n",
      "验证集：loss 0.696333, acc 0.680682, f1 0.678014\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 7>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [04:45, 16.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第7次迭代的损失为：0.52512354;准确率为：0.8198199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:03, 52.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.59      0.65      4399\n",
      "           1       0.65      0.77      0.71      4401\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      8800\n",
      "   macro avg       0.69      0.68      0.68      8800\n",
      "weighted avg       0.69      0.68      0.68      8800\n",
      "\n",
      "验证集：loss 0.696876, acc 0.680909, f1 0.67808\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 8>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [04:47, 16.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第8次迭代的损失为：0.51927036;准确率为：0.8251183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:03, 52.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.58      0.64      4399\n",
      "           1       0.65      0.78      0.71      4401\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      8800\n",
      "   macro avg       0.69      0.68      0.68      8800\n",
      "weighted avg       0.69      0.68      0.68      8800\n",
      "\n",
      "验证集：loss 0.7045, acc 0.678636, f1 0.675353\n",
      "\n",
      "training 9>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [04:47, 16.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第9次迭代的损失为：0.5156011;准确率为：0.82867855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:03, 52.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.58      0.65      4399\n",
      "           1       0.65      0.78      0.71      4401\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      8800\n",
      "   macro avg       0.69      0.68      0.68      8800\n",
      "weighted avg       0.69      0.68      0.68      8800\n",
      "\n",
      "验证集：loss 0.701377, acc 0.680568, f1 0.677502\n",
      "\n",
      "training 10>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [04:44, 16.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第10次迭代的损失为：0.50991726;准确率为：0.8337634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:04, 43.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.59      0.65      4399\n",
      "           1       0.65      0.77      0.71      4401\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      8800\n",
      "   macro avg       0.69      0.68      0.68      8800\n",
      "weighted avg       0.69      0.68      0.68      8800\n",
      "\n",
      "验证集：loss 0.70274, acc 0.681932, f1 0.6794\n",
      "\n",
      "Saved model success\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import re\n",
    "import jieba\n",
    "import random\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "class Data_Prepare(object):\n",
    "\n",
    "    def readfile(self, filename):\n",
    "        texta = []\n",
    "        textb = []\n",
    "        tag = []\n",
    "        with open(filename, 'r',encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip().split(\"\\t\")\n",
    "                texta.append(self.pre_processing(line[0]))\n",
    "                textb.append(self.pre_processing(line[1]))\n",
    "                tag.append(line[2])\n",
    "        # shuffle\n",
    "        index = [x for x in range(len(texta))]\n",
    "        random.shuffle(index)\n",
    "        texta_new = [texta[x] for x in index]\n",
    "        textb_new = [textb[x] for x in index]\n",
    "        tag_new = [tag[x] for x in index]\n",
    "        # 类别个数，构成[0,1]多分类向量\n",
    "        type = list(set(tag_new))\n",
    "        dicts = {}\n",
    "        tags_vec = []\n",
    "        for x in tag_new:\n",
    "            if x not in dicts.keys():\n",
    "                dicts[x] = 1\n",
    "            else:\n",
    "                dicts[x] += 1\n",
    "            temp = [0] * len(type)\n",
    "            temp[int(x)] = 1\n",
    "            tags_vec.append(temp)\n",
    "        print(dicts)\n",
    "        return texta_new, textb_new, tags_vec\n",
    "\n",
    "    def pre_processing(self, text):\n",
    "        # 删除（）里的内容\n",
    "        text = re.sub('（[^（.]*）', '', text)\n",
    "        # 只保留中文部分\n",
    "        text = ''.join([x for x in text if '\\u4e00' <= x <= '\\u9fa5'])\n",
    "        # 利用jieba进行分词\n",
    "        words = ' '.join(jieba.cut(text)).split(\" \")\n",
    "        # 不分词\n",
    "        words = [x for x in ''.join(words)]\n",
    "        return ' '.join(words)\n",
    "    # 构建词汇表\n",
    "    def build_vocab(self, sentences, path):\n",
    "        # 每个词的长度，取得最大长度，作为一个句子的固定长度，不足的补0\n",
    "        lens = [len(sentence.split(\" \")) for sentence in sentences]\n",
    "        max_length = max(lens)\n",
    "        vocab_processor = learn.preprocessing.VocabularyProcessor(max_length)\n",
    "        vocab_processor.fit(sentences)\n",
    "        vocab_processor.save(path)\n",
    "\n",
    "\n",
    "# -*- coding: UTF-8 -*-\n",
    "class Config(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.embedding_size = 50  # 词向量维度\n",
    "        self.hidden_num = 100  # 隐藏层规模\n",
    "        self.l2_lambda = 0.01\n",
    "        self.learning_rate = 0.001\n",
    "        self.dropout_keep_prob = 0.5\n",
    "        self.attn_size = 200\n",
    "        self.K = 2\n",
    "\n",
    "        self.epoch = 10\n",
    "        self.Batch_Size = 50\n",
    "        \n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "\n",
    "class Bi_lstm(object):\n",
    "\n",
    "    def __init__(self, is_trainning, seq_length, class_num, vocabulary_size, embedding_size, hidden_num,\n",
    "                l2_lambda, learning_rate):\n",
    "        self.is_trainning = is_trainning\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_num = hidden_num\n",
    "        self.seq_length = seq_length\n",
    "        self.class_num = class_num\n",
    "\n",
    "        # init placeholder\n",
    "        self.text_a = tf.placeholder(tf.int32, [None, seq_length], name='text_a')\n",
    "        self.text_b = tf.placeholder(tf.int32, [None, seq_length], name='text_b')\n",
    "        self.y = tf.placeholder(tf.int32, [None, class_num], name='y')\n",
    "        # real length\n",
    "        self.a_length = tf.placeholder(tf.int32, [None], name='a_length')\n",
    "        self.b_length = tf.placeholder(tf.int32, [None], name='b_length')\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # embedding层 论文中采用是预训练好的词向量 这里随机初始化一个词典 在训练过程中进行调整\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.vocab_matrix = tf.Variable(tf.truncated_normal(shape=[vocabulary_size, embedding_size],\n",
    "                                                                stddev=1.0 / math.sqrt(embedding_size)),\n",
    "                                            name='vacab_matrix')\n",
    "            self.text_a_embed = tf.nn.embedding_lookup(self.vocab_matrix, self.text_a)\n",
    "            self.text_b_embed = tf.nn.embedding_lookup(self.vocab_matrix, self.text_b)\n",
    "\n",
    "        with tf.name_scope('Input_Encoding'):\n",
    "            a = self.biLSTMBlock(self.text_a_embed, hidden_num, 'Input_Encoding/biLSTM', self.a_length)\n",
    "            b = self.biLSTMBlock(self.text_b_embed, hidden_num, 'Input_Encoding/biLSTM', self.b_length, isreuse=True)\n",
    "        # a-b\n",
    "        diff = tf.subtract(a, b)\n",
    "\n",
    "        with tf.name_scope(\"output\"):\n",
    "            initializer = tf.random_normal_initializer(0.0, 0.1)\n",
    "            with tf.variable_scope('feed_foward'):\n",
    "                outputs = tf.nn.dropout(diff, self.dropout_keep_prob)\n",
    "                outputs = tf.reshape(outputs, [-1, hidden_num * seq_length * 2])\n",
    "                outputs = tf.layers.dense(outputs, hidden_num, tf.nn.relu, kernel_initializer=initializer)\n",
    "                self.logits = tf.layers.dense(outputs, class_num, tf.nn.tanh, kernel_initializer=initializer)\n",
    "            self.score = tf.nn.softmax(self.logits, name='score')\n",
    "            self.prediction = tf.argmax(self.score, 1, name=\"prediction\")\n",
    "\n",
    "        with tf.name_scope('cost'):\n",
    "            self.cost = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.y, logits=self.logits)\n",
    "            self.cost = tf.reduce_mean(self.cost)\n",
    "            weights = [v for v in tf.trainable_variables() if ('w' in v.name) or ('kernel' in v.name)]\n",
    "            l2_loss = tf.add_n([tf.nn.l2_loss(w) for w in weights]) * l2_lambda\n",
    "            self.loss = l2_loss + self.cost\n",
    "\n",
    "        self.accuracy = tf.reduce_mean(\n",
    "            tf.cast(tf.equal(tf.argmax(self.y, axis=1), self.prediction), tf.float32))\n",
    "\n",
    "        if not is_trainning:\n",
    "            return\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), 5)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    def biLSTMBlock(self, inputs, num_units, scope, seq_len=None, isreuse=False):\n",
    "        with tf.variable_scope(scope, reuse=isreuse):\n",
    "            cell_fw = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "            fw_lstm_cell = tf.contrib.rnn.DropoutWrapper(cell_fw, output_keep_prob=self.dropout_keep_prob)\n",
    "            cell_bw = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "            bw_lstm_cell = tf.contrib.rnn.DropoutWrapper(cell_bw, output_keep_prob=self.dropout_keep_prob)\n",
    "\n",
    "            (a_outputs, a_output_states) = tf.nn.bidirectional_dynamic_rnn(fw_lstm_cell, bw_lstm_cell,\n",
    "                                                                           inputs,\n",
    "                                                                           sequence_length=seq_len,\n",
    "                                                                           dtype=tf.float32)\n",
    "            a_bar = tf.concat(a_outputs, axis=2)\n",
    "            return a_bar\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "\n",
    "class Bi_lstm_Atten(object):\n",
    "\n",
    "    def __init__(self, is_trainning, seq_length, class_num, vocabulary_size, embedding_size, hidden_num,\n",
    "                 attn_size, l2_lambda, learning_rate):\n",
    "        self.is_trainning = is_trainning\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_num = hidden_num\n",
    "        self.attn_size = attn_size\n",
    "        self.seq_length = seq_length\n",
    "        self.class_num = class_num\n",
    "\n",
    "        # init placeholder\n",
    "        self.text_a = tf.placeholder(tf.int32, [None, seq_length], name='text_a')\n",
    "        self.text_b = tf.placeholder(tf.int32, [None, seq_length], name='text_b')\n",
    "        self.y = tf.placeholder(tf.int32, [None, class_num], name='y')\n",
    "        # real length\n",
    "        self.a_length = tf.placeholder(tf.int32, [None], name='a_length')\n",
    "        self.b_length = tf.placeholder(tf.int32, [None], name='b_length')\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # embedding层 论文中采用是预训练好的词向量 这里随机初始化一个词典 在训练过程中进行调整\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.vocab_matrix = tf.Variable(tf.truncated_normal(shape=[vocabulary_size, embedding_size],\n",
    "                                                                stddev=1.0 / math.sqrt(embedding_size)),\n",
    "                                            name='vacab_matrix')\n",
    "            self.text_a_embed = tf.nn.embedding_lookup(self.vocab_matrix, self.text_a)\n",
    "            self.text_b_embed = tf.nn.embedding_lookup(self.vocab_matrix, self.text_b)\n",
    "\n",
    "        with tf.name_scope('Input_Encoding'):\n",
    "            a = self.biLSTMBlock(self.text_a_embed, hidden_num, 'Input_Encoding/biLSTM', self.a_length)\n",
    "            a_atten = self.attention(self.attn_size, a, hidden_num, seq_length, self.a_length)\n",
    "            b = self.biLSTMBlock(self.text_b_embed, hidden_num, 'Input_Encoding/biLSTM', self.b_length, isreuse=True)\n",
    "            b_atten = self.attention(self.attn_size, b, hidden_num, seq_length, self.b_length)\n",
    "\n",
    "        diff = tf.subtract(a_atten, b_atten)\n",
    "\n",
    "        with tf.name_scope(\"output\"):\n",
    "            initializer = tf.random_normal_initializer(0.0, 0.1)\n",
    "            with tf.variable_scope('feed_foward'):\n",
    "                outputs = tf.nn.dropout(diff, self.dropout_keep_prob)\n",
    "                self.logits = tf.layers.dense(outputs, class_num, tf.nn.tanh, kernel_initializer=initializer)\n",
    "            self.score = tf.nn.softmax(self.logits, name='score')\n",
    "            self.prediction = tf.argmax(self.score, 1, name=\"prediction\")\n",
    "\n",
    "        with tf.name_scope('cost'):\n",
    "            self.cost = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.y, logits=self.logits)\n",
    "            self.cost = tf.reduce_mean(self.cost)\n",
    "            weights = [v for v in tf.trainable_variables() if ('w' in v.name) or ('kernel' in v.name)]\n",
    "            l2_loss = tf.add_n([tf.nn.l2_loss(w) for w in weights]) * l2_lambda\n",
    "            self.loss = l2_loss + self.cost\n",
    "\n",
    "        self.accuracy = tf.reduce_mean(\n",
    "            tf.cast(tf.equal(tf.argmax(self.y, axis=1), self.prediction), tf.float32))\n",
    "\n",
    "        if not is_trainning:\n",
    "            return\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), 5)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    def biLSTMBlock(self, inputs, num_units, scope, seq_len=None, isreuse=False):\n",
    "        with tf.variable_scope(scope, reuse=isreuse):\n",
    "            cell_fw = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "            fw_lstm_cell = tf.contrib.rnn.DropoutWrapper(cell_fw, output_keep_prob=self.dropout_keep_prob)\n",
    "            cell_bw = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "            bw_lstm_cell = tf.contrib.rnn.DropoutWrapper(cell_bw, output_keep_prob=self.dropout_keep_prob)\n",
    "\n",
    "            (a_outputs, a_output_states) = tf.nn.bidirectional_dynamic_rnn(fw_lstm_cell, bw_lstm_cell,\n",
    "                                                                           inputs,\n",
    "                                                                           sequence_length=seq_len,\n",
    "                                                                           dtype=tf.float32)\n",
    "            a_bar = tf.concat(a_outputs, axis=2)\n",
    "            return a_bar\n",
    "\n",
    "    def attention(self, attn_size, outputs, hidden_num, max_length, length):\n",
    "        # attention\n",
    "        attention_size = attn_size\n",
    "        outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "        with tf.name_scope('attention'), tf.variable_scope('attention'):\n",
    "            attention_w = tf.Variable(tf.truncated_normal([2 * hidden_num, attention_size], stddev=0.1),\n",
    "                                      name='attention_w')\n",
    "            attention_b = tf.Variable(tf.constant(0.1, shape=[attention_size]), name='attention_b')\n",
    "            u_list = []\n",
    "            for t in range(max_length):\n",
    "                u_t = tf.tanh(tf.matmul(outputs[t], attention_w) + attention_b)\n",
    "                u_list.append(u_t)\n",
    "            u_w = tf.Variable(tf.truncated_normal([attention_size, 1], stddev=0.1), name='attention_uw')\n",
    "            attn_z = []\n",
    "            for t in range(max_length):\n",
    "                z_t = tf.matmul(u_list[t], u_w)\n",
    "                attn_z.append(z_t)\n",
    "            # transform to batch_size * sequence_length\n",
    "            attn_zconcat = tf.concat(attn_z, axis=1)\n",
    "            # masked\n",
    "            attn_zconcat = self.mask(attn_zconcat, length, max_length)\n",
    "            self.alpha = tf.nn.softmax(attn_zconcat)\n",
    "            # transform to sequence_length * batch_size * 1 , same rank as outputs\n",
    "            alpha_trans = tf.expand_dims(self.alpha, -1)\n",
    "\n",
    "        final_output = tf.reduce_sum(tf.transpose(outputs, [1, 0, 2]) * alpha_trans, 1)\n",
    "        return final_output\n",
    "\n",
    "    def mask(self, inputs, seq_len, max_len):\n",
    "        mask = tf.cast(tf.sequence_mask(seq_len, maxlen=max_len), tf.float32)\n",
    "        return inputs - (1 - mask) * 1e12\n",
    "\n",
    "\n",
    "    \n",
    "import tensorflow as tf\n",
    "import data_prepare\n",
    "from tensorflow.contrib import learn\n",
    "import numpy as np\n",
    "import bi_lstm\n",
    "import config as config\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "import os\n",
    "\n",
    "con = config.Config()\n",
    "parent_path = os.path.dirname(os.getcwd())\n",
    "data_pre = data_prepare.Data_Prepare()\n",
    "\n",
    "\n",
    "class TrainModel(object):\n",
    "    '''\n",
    "        训练模型\n",
    "        保存模型\n",
    "    '''\n",
    "    def pre_processing(self):\n",
    "\n",
    "        train_texta, train_textb, train_tag = data_pre.readfile('data/train.txt')\n",
    "        data = []\n",
    "        data.extend(train_texta)\n",
    "        data.extend(train_textb)\n",
    "        # 训练集构造词典，并且保存\n",
    "        data_pre.build_vocab(data, 'save_model/vocab/vocab.pickle')\n",
    "        # 加载词典\n",
    "        self.vocab_processor = learn.preprocessing.VocabularyProcessor.restore('save_model/vocab/vocab.pickle')\n",
    "        # 中文句子，转化为id序列，固定长度\n",
    "        train_texta_embedding = np.array(list(self.vocab_processor.transform(train_texta)))\n",
    "        train_textb_embedding = np.array(list(self.vocab_processor.transform(train_textb)))\n",
    "        print('train_texta_embedding:', train_texta_embedding[:5])\n",
    "        print('train_textb_embedding:', train_textb_embedding[:5])\n",
    "\n",
    "\n",
    "        dev_texta, dev_textb, dev_tag = data_pre.readfile('data/dev.txt')\n",
    "        dev_texta_embedding = np.array(list(self.vocab_processor.transform(dev_texta)))\n",
    "        dev_textb_embedding = np.array(list(self.vocab_processor.transform(dev_textb)))\n",
    "        return train_texta_embedding, train_textb_embedding, np.array(train_tag), \\\n",
    "               dev_texta_embedding, dev_textb_embedding, np.array(dev_tag)\n",
    "\n",
    "    def get_batches(self, texta, textb, tag):\n",
    "        num_batch = int(len(texta) / con.Batch_Size)\n",
    "        for i in range(num_batch):\n",
    "            a = texta[i*con.Batch_Size:(i+1)*con.Batch_Size]\n",
    "            b = textb[i*con.Batch_Size:(i+1)*con.Batch_Size]\n",
    "            t = tag[i*con.Batch_Size:(i+1)*con.Batch_Size]\n",
    "            yield a, b, t\n",
    "\n",
    "    def get_length(self, trainX_batch):\n",
    "        # sentence length\n",
    "        lengths = []\n",
    "        for sample in trainX_batch:\n",
    "            count = 0\n",
    "            for index in sample:\n",
    "                if index != 0:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "            lengths.append(count)\n",
    "        return lengths\n",
    "\n",
    "    def trainModel(self):\n",
    "        train_texta_embedding, train_textb_embedding, train_tag, \\\n",
    "        dev_texta_embedding, dev_textb_embedding, dev_tag = self.pre_processing()\n",
    "        # 定义训练用的循环神经网络模型\n",
    "        with tf.variable_scope('bi_listm_model', reuse=None):\n",
    "            # bi_listm\n",
    "            model = Bi_lstm(True, seq_length=len(train_texta_embedding[0]),\n",
    "                                    class_num=len(train_tag[0]),\n",
    "                                    vocabulary_size=len(self.vocab_processor.vocabulary_),\n",
    "                                    embedding_size=con.embedding_size,\n",
    "                                    hidden_num=con.hidden_num,\n",
    "                                    l2_lambda=con.l2_lambda,\n",
    "                                    learning_rate=con.learning_rate)\n",
    "\n",
    "        # 训练模型\n",
    "        with tf.Session() as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            saver = tf.train.Saver()\n",
    "            best_f1 = 0.0\n",
    "            for time in range(con.epoch):\n",
    "                print(\"training \" + str(time + 1) + \">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "                model.is_trainning = True\n",
    "                loss_all = []\n",
    "                accuracy_all = []\n",
    "                for texta, textb, tag in tqdm(self.get_batches(train_texta_embedding, train_textb_embedding, train_tag)):\n",
    "                    feed_dict = {\n",
    "                        model.text_a: texta,\n",
    "                        model.text_b: textb,\n",
    "                        model.y: tag,\n",
    "                        model.dropout_keep_prob: con.dropout_keep_prob,\n",
    "                        model.a_length: np.array(self.get_length(texta)),\n",
    "                        model.b_length: np.array(self.get_length(textb))\n",
    "                    }\n",
    "                    _, cost, accuracy = sess.run([model.train_op, model.loss, model.accuracy], feed_dict)\n",
    "                    loss_all.append(cost)\n",
    "                    accuracy_all.append(accuracy)\n",
    "\n",
    "                print(\"第\" + str((time + 1)) + \"次迭代的损失为：\" + str(np.mean(np.array(loss_all))) + \";准确率为：\" +\n",
    "                      str(np.mean(np.array(accuracy_all))))\n",
    "\n",
    "                def dev_step():\n",
    "                    \"\"\"\n",
    "                    Evaluates model on a dev set\n",
    "                    \"\"\"\n",
    "                    loss_all = []\n",
    "                    accuracy_all = []\n",
    "                    predictions = []\n",
    "                    for texta, textb, tag in tqdm(self.get_batches(dev_texta_embedding, dev_textb_embedding, dev_tag)):\n",
    "                        feed_dict = {\n",
    "                            model.text_a: texta,\n",
    "                            model.text_b: textb,\n",
    "                            model.y: tag,\n",
    "                            model.dropout_keep_prob: 1.0,\n",
    "                            model.a_length: np.array(self.get_length(texta)),\n",
    "                            model.b_length: np.array(self.get_length(textb))\n",
    "                        }\n",
    "                        dev_cost, dev_accuracy, prediction = sess.run([model.loss, model.accuracy,\n",
    "                                                                       model.prediction], feed_dict)\n",
    "                        loss_all.append(dev_cost)\n",
    "                        accuracy_all.append(dev_accuracy)\n",
    "                        predictions.extend(prediction)\n",
    "                    y_true = [np.nonzero(x)[0][0] for x in dev_tag]\n",
    "                    y_true = y_true[0:len(loss_all)*con.Batch_Size]\n",
    "                    f1 = f1_score(np.array(y_true), np.array(predictions), average='weighted')\n",
    "                    print('分类报告:\\n', metrics.classification_report(np.array(y_true), predictions))\n",
    "                    print(\"验证集：loss {:g}, acc {:g}, f1 {:g}\\n\".format(np.mean(np.array(loss_all)),\n",
    "                                                                      np.mean(np.array(accuracy_all)), f1))\n",
    "                    return f1\n",
    "\n",
    "                model.is_trainning = False\n",
    "                f1 = dev_step()\n",
    "\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    saver.save(sess, \"save_model/ckpt/model.ckpt\")\n",
    "                    print(\"Saved model success\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train = TrainModel()\n",
    "    train.trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\nie\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 2.232 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 138574, '0': 100192}\n",
      "WARNING:tensorflow:From E:\\python_workSpace\\text_match\\LSTM\\data_prepare.py:55: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "train_texta_embedding: [[ 1  2  3  4  5  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 7  8  9 10 11 12 13 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0]\n",
      " [15 16 17 18 19 20 21 22 23 24  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0]\n",
      " [25 26 27 28 29 30 31 32 33 34 35 20 21 35 36 37  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0]\n",
      " [38 39 40 41 21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0]]\n",
      "train_textb_embedding: [[   4    5    6   52    1    2  430    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0]\n",
      " [   7    8   12   13   59   60   10   11   14    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0]\n",
      " [  20  238  212  213  325  326  580  581   20  238   52   17   18    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0]\n",
      " [  25   26   27   28   33   34   35  776   40  394  513   36    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0]\n",
      " [  38   39   65   41   21   76  254   21 1291    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0]]\n",
      "{'1': 4402, '0': 4400}\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-1-8e3ec146f457>:147: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-1-8e3ec146f457>:155: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-1-8e3ec146f457>:122: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "training 1>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [23:30,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次迭代的损失为：0.72725546;准确率为：0.5803979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:18,  9.46it/s]\n",
      "e:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "e:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      4399\n",
      "           1       0.50      1.00      0.67      4401\n",
      "\n",
      "   micro avg       0.50      0.50      0.50      8800\n",
      "   macro avg       0.25      0.50      0.33      8800\n",
      "weighted avg       0.25      0.50      0.33      8800\n",
      "\n",
      "验证集：loss 0.707106, acc 0.500114, f1 0.33346\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 2>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [22:31,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2次迭代的损失为：0.6805064;准确率为：0.580377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:12, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      4399\n",
      "           1       0.50      1.00      0.67      4401\n",
      "\n",
      "   micro avg       0.50      0.50      0.50      8800\n",
      "   macro avg       0.25      0.50      0.33      8800\n",
      "weighted avg       0.25      0.50      0.33      8800\n",
      "\n",
      "验证集：loss 0.707114, acc 0.500114, f1 0.33346\n",
      "\n",
      "training 3>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [23:18,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3次迭代的损失为：0.6805195;准确率为：0.580377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:15, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      4399\n",
      "           1       0.50      1.00      0.67      4401\n",
      "\n",
      "   micro avg       0.50      0.50      0.50      8800\n",
      "   macro avg       0.25      0.50      0.33      8800\n",
      "weighted avg       0.25      0.50      0.33      8800\n",
      "\n",
      "验证集：loss 0.707135, acc 0.500114, f1 0.33346\n",
      "\n",
      "training 4>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [20:31,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第4次迭代的损失为：0.68070984;准确率为：0.5804398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:13, 13.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      4399\n",
      "           1       0.50      1.00      0.67      4401\n",
      "\n",
      "   micro avg       0.50      0.50      0.50      8800\n",
      "   macro avg       0.25      0.50      0.33      8800\n",
      "weighted avg       0.25      0.50      0.33      8800\n",
      "\n",
      "验证集：loss 0.708205, acc 0.500114, f1 0.33346\n",
      "\n",
      "training 5>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [20:52,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第5次迭代的损失为：0.680059;准确率为：0.6019979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:12, 13.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.03      0.06      4399\n",
      "           1       0.51      0.99      0.67      4401\n",
      "\n",
      "   micro avg       0.51      0.51      0.51      8800\n",
      "   macro avg       0.64      0.51      0.36      8800\n",
      "weighted avg       0.64      0.51      0.36      8800\n",
      "\n",
      "验证集：loss 0.714895, acc 0.510909, f1 0.364736\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 6>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [20:51,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第6次迭代的损失为：0.67720157;准确率为：0.6328167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:12, 13.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.10      0.17      4399\n",
      "           1       0.52      0.97      0.67      4401\n",
      "\n",
      "   micro avg       0.53      0.53      0.53      8800\n",
      "   macro avg       0.63      0.53      0.42      8800\n",
      "weighted avg       0.63      0.53      0.42      8800\n",
      "\n",
      "验证集：loss 0.719451, acc 0.532727, f1 0.424149\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 7>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [20:29,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第7次迭代的损失为：0.6743023;准确率为：0.65105754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:13, 12.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.15      0.25      4399\n",
      "           1       0.53      0.94      0.68      4401\n",
      "\n",
      "   micro avg       0.55      0.55      0.55      8800\n",
      "   macro avg       0.63      0.55      0.46      8800\n",
      "weighted avg       0.63      0.55      0.46      8800\n",
      "\n",
      "验证集：loss 0.724406, acc 0.546136, f1 0.460435\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 8>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [20:50,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第8次迭代的损失为：0.6671057;准确率为：0.66862404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:13, 13.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.35      0.40      4399\n",
      "           1       0.49      0.62      0.55      4401\n",
      "\n",
      "   micro avg       0.49      0.49      0.49      8800\n",
      "   macro avg       0.48      0.49      0.48      8800\n",
      "weighted avg       0.48      0.49      0.48      8800\n",
      "\n",
      "验证集：loss 0.795602, acc 0.486136, f1 0.47615\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 9>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [20:34,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第9次迭代的损失为：0.6532403;准确率为：0.6851769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:13, 11.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.42      0.46      4399\n",
      "           1       0.50      0.59      0.54      4401\n",
      "\n",
      "   micro avg       0.50      0.50      0.50      8800\n",
      "   macro avg       0.50      0.50      0.50      8800\n",
      "weighted avg       0.50      0.50      0.50      8800\n",
      "\n",
      "验证集：loss 0.812499, acc 0.503636, f1 0.500092\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 10>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [20:36,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第10次迭代的损失为：0.64361686;准确率为：0.69863456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:12, 14.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.42      0.46      4399\n",
      "           1       0.51      0.61      0.55      4401\n",
      "\n",
      "   micro avg       0.51      0.51      0.51      8800\n",
      "   macro avg       0.51      0.51      0.51      8800\n",
      "weighted avg       0.51      0.51      0.51      8800\n",
      "\n",
      "验证集：loss 0.817204, acc 0.511932, f1 0.507357\n",
      "\n",
      "Saved model success\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import re\n",
    "import jieba\n",
    "import random\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "class Data_Prepare(object):\n",
    "\n",
    "    def readfile(self, filename):\n",
    "        texta = []\n",
    "        textb = []\n",
    "        tag = []\n",
    "        with open(filename, 'r',encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip().split(\"\\t\")\n",
    "                texta.append(self.pre_processing(line[0]))\n",
    "                textb.append(self.pre_processing(line[1]))\n",
    "                tag.append(line[2])\n",
    "        # shuffle\n",
    "        index = [x for x in range(len(texta))]\n",
    "        random.shuffle(index)\n",
    "        texta_new = [texta[x] for x in index]\n",
    "        textb_new = [textb[x] for x in index]\n",
    "        tag_new = [tag[x] for x in index]\n",
    "        # 类别个数，构成[0,1]多分类向量\n",
    "        type = list(set(tag_new))\n",
    "        dicts = {}\n",
    "        tags_vec = []\n",
    "        for x in tag_new:\n",
    "            if x not in dicts.keys():\n",
    "                dicts[x] = 1\n",
    "            else:\n",
    "                dicts[x] += 1\n",
    "            temp = [0] * len(type)\n",
    "            temp[int(x)] = 1\n",
    "            tags_vec.append(temp)\n",
    "        print(dicts)\n",
    "        return texta_new, textb_new, tags_vec\n",
    "\n",
    "    def pre_processing(self, text):\n",
    "        # 删除（）里的内容\n",
    "        text = re.sub('（[^（.]*）', '', text)\n",
    "        # 只保留中文部分\n",
    "        text = ''.join([x for x in text if '\\u4e00' <= x <= '\\u9fa5'])\n",
    "        # 利用jieba进行分词\n",
    "        words = ' '.join(jieba.cut(text)).split(\" \")\n",
    "        # 不分词\n",
    "        words = [x for x in ''.join(words)]\n",
    "        return ' '.join(words)\n",
    "    # 构建词汇表\n",
    "    def build_vocab(self, sentences, path):\n",
    "        # 每个词的长度，取得最大长度，作为一个句子的固定长度，不足的补0\n",
    "        lens = [len(sentence.split(\" \")) for sentence in sentences]\n",
    "        max_length = max(lens)\n",
    "        vocab_processor = learn.preprocessing.VocabularyProcessor(max_length)\n",
    "        vocab_processor.fit(sentences)\n",
    "        vocab_processor.save(path)\n",
    "\n",
    "\n",
    "# -*- coding: UTF-8 -*-\n",
    "class Config(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.embedding_size = 50  # 词向量维度\n",
    "        self.hidden_num = 100  # 隐藏层规模\n",
    "        self.l2_lambda = 0.01\n",
    "        self.learning_rate = 0.001\n",
    "        self.dropout_keep_prob = 0.5\n",
    "        self.attn_size = 200\n",
    "        self.K = 2\n",
    "\n",
    "        self.epoch = 10\n",
    "        self.Batch_Size = 50\n",
    "        \n",
    "\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "\n",
    "class Bi_lstm_Atten(object):\n",
    "\n",
    "    def __init__(self, is_trainning, seq_length, class_num, vocabulary_size, embedding_size, hidden_num,\n",
    "                 attn_size, l2_lambda, learning_rate):\n",
    "        self.is_trainning = is_trainning\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_num = hidden_num\n",
    "        self.attn_size = attn_size\n",
    "        self.seq_length = seq_length\n",
    "        self.class_num = class_num\n",
    "\n",
    "        # init placeholder\n",
    "        self.text_a = tf.placeholder(tf.int32, [None, seq_length], name='text_a')\n",
    "        self.text_b = tf.placeholder(tf.int32, [None, seq_length], name='text_b')\n",
    "        self.y = tf.placeholder(tf.int32, [None, class_num], name='y')\n",
    "        # real length\n",
    "        self.a_length = tf.placeholder(tf.int32, [None], name='a_length')\n",
    "        self.b_length = tf.placeholder(tf.int32, [None], name='b_length')\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # embedding层 论文中采用是预训练好的词向量 这里随机初始化一个词典 在训练过程中进行调整\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.vocab_matrix = tf.Variable(tf.truncated_normal(shape=[vocabulary_size, embedding_size],\n",
    "                                                                stddev=1.0 / math.sqrt(embedding_size)),\n",
    "                                            name='vacab_matrix')\n",
    "            self.text_a_embed = tf.nn.embedding_lookup(self.vocab_matrix, self.text_a)\n",
    "            self.text_b_embed = tf.nn.embedding_lookup(self.vocab_matrix, self.text_b)\n",
    "\n",
    "        with tf.name_scope('Input_Encoding'):\n",
    "            a = self.biLSTMBlock(self.text_a_embed, hidden_num, 'Input_Encoding/biLSTM', self.a_length)\n",
    "            # 加了attention\n",
    "            a_atten = self.attention(self.attn_size, a, hidden_num, seq_length, self.a_length)\n",
    "            b = self.biLSTMBlock(self.text_b_embed, hidden_num, 'Input_Encoding/biLSTM', self.b_length, isreuse=True)\n",
    "            b_atten = self.attention(self.attn_size, b, hidden_num, seq_length, self.b_length)\n",
    "\n",
    "        diff = tf.subtract(a_atten, b_atten)\n",
    "\n",
    "        with tf.name_scope(\"output\"):\n",
    "            initializer = tf.random_normal_initializer(0.0, 0.1)\n",
    "            with tf.variable_scope('feed_foward'):\n",
    "                outputs = tf.nn.dropout(diff, self.dropout_keep_prob)\n",
    "                self.logits = tf.layers.dense(outputs, class_num, tf.nn.tanh, kernel_initializer=initializer)\n",
    "            self.score = tf.nn.softmax(self.logits, name='score')\n",
    "            self.prediction = tf.argmax(self.score, 1, name=\"prediction\")\n",
    "\n",
    "        with tf.name_scope('cost'):\n",
    "            self.cost = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.y, logits=self.logits)\n",
    "            self.cost = tf.reduce_mean(self.cost)\n",
    "            weights = [v for v in tf.trainable_variables() if ('w' in v.name) or ('kernel' in v.name)]\n",
    "            l2_loss = tf.add_n([tf.nn.l2_loss(w) for w in weights]) * l2_lambda\n",
    "            self.loss = l2_loss + self.cost\n",
    "\n",
    "        self.accuracy = tf.reduce_mean(\n",
    "            tf.cast(tf.equal(tf.argmax(self.y, axis=1), self.prediction), tf.float32))\n",
    "\n",
    "        if not is_trainning:\n",
    "            return\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), 5)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    def biLSTMBlock(self, inputs, num_units, scope, seq_len=None, isreuse=False):\n",
    "        with tf.variable_scope(scope, reuse=isreuse):\n",
    "            cell_fw = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "            fw_lstm_cell = tf.contrib.rnn.DropoutWrapper(cell_fw, output_keep_prob=self.dropout_keep_prob)\n",
    "            cell_bw = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "            bw_lstm_cell = tf.contrib.rnn.DropoutWrapper(cell_bw, output_keep_prob=self.dropout_keep_prob)\n",
    "\n",
    "            (a_outputs, a_output_states) = tf.nn.bidirectional_dynamic_rnn(fw_lstm_cell, bw_lstm_cell,\n",
    "                                                                           inputs,\n",
    "                                                                           sequence_length=seq_len,\n",
    "                                                                           dtype=tf.float32)\n",
    "            a_bar = tf.concat(a_outputs, axis=2)\n",
    "            return a_bar\n",
    "\n",
    "    def attention(self, attn_size, outputs, hidden_num, max_length, length):\n",
    "        # attention\n",
    "        attention_size = attn_size\n",
    "        outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "        with tf.name_scope('attention'), tf.variable_scope('attention'):\n",
    "            attention_w = tf.Variable(tf.truncated_normal([2 * hidden_num, attention_size], stddev=0.1),\n",
    "                                      name='attention_w')\n",
    "            attention_b = tf.Variable(tf.constant(0.1, shape=[attention_size]), name='attention_b')\n",
    "            u_list = []\n",
    "            for t in range(max_length):\n",
    "                u_t = tf.tanh(tf.matmul(outputs[t], attention_w) + attention_b)\n",
    "                u_list.append(u_t)\n",
    "            u_w = tf.Variable(tf.truncated_normal([attention_size, 1], stddev=0.1), name='attention_uw')\n",
    "            attn_z = []\n",
    "            for t in range(max_length):\n",
    "                z_t = tf.matmul(u_list[t], u_w)\n",
    "                attn_z.append(z_t)\n",
    "            # transform to batch_size * sequence_length\n",
    "            attn_zconcat = tf.concat(attn_z, axis=1)\n",
    "            # masked\n",
    "            attn_zconcat = self.mask(attn_zconcat, length, max_length)\n",
    "            self.alpha = tf.nn.softmax(attn_zconcat)\n",
    "            # transform to sequence_length * batch_size * 1 , same rank as outputs\n",
    "            alpha_trans = tf.expand_dims(self.alpha, -1)\n",
    "\n",
    "        final_output = tf.reduce_sum(tf.transpose(outputs, [1, 0, 2]) * alpha_trans, 1)\n",
    "        return final_output\n",
    "\n",
    "    def mask(self, inputs, seq_len, max_len):\n",
    "        mask = tf.cast(tf.sequence_mask(seq_len, maxlen=max_len), tf.float32)\n",
    "        return inputs - (1 - mask) * 1e12\n",
    "\n",
    "\n",
    "    \n",
    "import tensorflow as tf\n",
    "import data_prepare\n",
    "from tensorflow.contrib import learn\n",
    "import numpy as np\n",
    "import bi_lstm\n",
    "import config as config\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "import os\n",
    "\n",
    "con = config.Config()\n",
    "parent_path = os.path.dirname(os.getcwd())\n",
    "data_pre = data_prepare.Data_Prepare()\n",
    "\n",
    "\n",
    "class TrainModel(object):\n",
    "    '''\n",
    "        训练模型\n",
    "        保存模型\n",
    "    '''\n",
    "    def pre_processing(self):\n",
    "\n",
    "        train_texta, train_textb, train_tag = data_pre.readfile('data/train.txt')\n",
    "        data = []\n",
    "        data.extend(train_texta)\n",
    "        data.extend(train_textb)\n",
    "        # 训练集构造词典，并且保存\n",
    "        data_pre.build_vocab(data, 'save_model/vocab/vocab.pickle')\n",
    "        # 加载词典\n",
    "        self.vocab_processor = learn.preprocessing.VocabularyProcessor.restore('save_model/vocab/vocab.pickle')\n",
    "        # 中文句子，转化为id序列，固定长度\n",
    "        train_texta_embedding = np.array(list(self.vocab_processor.transform(train_texta)))\n",
    "        train_textb_embedding = np.array(list(self.vocab_processor.transform(train_textb)))\n",
    "        print('train_texta_embedding:', train_texta_embedding[:5])\n",
    "        print('train_textb_embedding:', train_textb_embedding[:5])\n",
    "\n",
    "\n",
    "        dev_texta, dev_textb, dev_tag = data_pre.readfile('data/dev.txt')\n",
    "        dev_texta_embedding = np.array(list(self.vocab_processor.transform(dev_texta)))\n",
    "        dev_textb_embedding = np.array(list(self.vocab_processor.transform(dev_textb)))\n",
    "        return train_texta_embedding, train_textb_embedding, np.array(train_tag), \\\n",
    "               dev_texta_embedding, dev_textb_embedding, np.array(dev_tag)\n",
    "\n",
    "    def get_batches(self, texta, textb, tag):\n",
    "        num_batch = int(len(texta) / con.Batch_Size)\n",
    "        for i in range(num_batch):\n",
    "            a = texta[i*con.Batch_Size:(i+1)*con.Batch_Size]\n",
    "            b = textb[i*con.Batch_Size:(i+1)*con.Batch_Size]\n",
    "            t = tag[i*con.Batch_Size:(i+1)*con.Batch_Size]\n",
    "            yield a, b, t\n",
    "\n",
    "    def get_length(self, trainX_batch):\n",
    "        # sentence length\n",
    "        lengths = []\n",
    "        for sample in trainX_batch:\n",
    "            count = 0\n",
    "            for index in sample:\n",
    "                if index != 0:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "            lengths.append(count)\n",
    "        return lengths\n",
    "\n",
    "    def trainModel(self):\n",
    "        train_texta_embedding, train_textb_embedding, train_tag, \\\n",
    "        dev_texta_embedding, dev_textb_embedding, dev_tag = self.pre_processing()\n",
    "        # 定义训练用的循环神经网络模型\n",
    "        with tf.variable_scope('bi_listm_model', reuse=None):\n",
    "            # bi_listm_attn\n",
    "            model = Bi_lstm_Atten(True, seq_length=len(train_texta_embedding[0]),\n",
    "                                    class_num=len(train_tag[0]),\n",
    "                                    vocabulary_size=len(self.vocab_processor.vocabulary_),\n",
    "                                    embedding_size=con.embedding_size,\n",
    "                                    hidden_num=con.hidden_num,\n",
    "                                    attn_size = con.attn_size,\n",
    "                                    l2_lambda=con.l2_lambda,\n",
    "                                    learning_rate=con.learning_rate)\n",
    "\n",
    "        # 训练模型\n",
    "        with tf.Session() as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            saver = tf.train.Saver()\n",
    "            best_f1 = 0.0\n",
    "            for time in range(con.epoch):\n",
    "                print(\"training \" + str(time + 1) + \">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "                model.is_trainning = True\n",
    "                loss_all = []\n",
    "                accuracy_all = []\n",
    "                for texta, textb, tag in tqdm(self.get_batches(train_texta_embedding, train_textb_embedding, train_tag)):\n",
    "                    feed_dict = {\n",
    "                        model.text_a: texta,\n",
    "                        model.text_b: textb,\n",
    "                        model.y: tag,\n",
    "                        model.dropout_keep_prob: con.dropout_keep_prob,\n",
    "                        model.a_length: np.array(self.get_length(texta)),\n",
    "                        model.b_length: np.array(self.get_length(textb))\n",
    "                    }\n",
    "                    _, cost, accuracy = sess.run([model.train_op, model.loss, model.accuracy], feed_dict)\n",
    "                    loss_all.append(cost)\n",
    "                    accuracy_all.append(accuracy)\n",
    "\n",
    "                print(\"第\" + str((time + 1)) + \"次迭代的损失为：\" + str(np.mean(np.array(loss_all))) + \";准确率为：\" +\n",
    "                      str(np.mean(np.array(accuracy_all))))\n",
    "\n",
    "                def dev_step():\n",
    "                    \"\"\"\n",
    "                    Evaluates model on a dev set\n",
    "                    \"\"\"\n",
    "                    loss_all = []\n",
    "                    accuracy_all = []\n",
    "                    predictions = []\n",
    "                    for texta, textb, tag in tqdm(self.get_batches(dev_texta_embedding, dev_textb_embedding, dev_tag)):\n",
    "                        feed_dict = {\n",
    "                            model.text_a: texta,\n",
    "                            model.text_b: textb,\n",
    "                            model.y: tag,\n",
    "                            model.dropout_keep_prob: 1.0,\n",
    "                            model.a_length: np.array(self.get_length(texta)),\n",
    "                            model.b_length: np.array(self.get_length(textb))\n",
    "                        }\n",
    "                        dev_cost, dev_accuracy, prediction = sess.run([model.loss, model.accuracy,\n",
    "                                                                       model.prediction], feed_dict)\n",
    "                        loss_all.append(dev_cost)\n",
    "                        accuracy_all.append(dev_accuracy)\n",
    "                        predictions.extend(prediction)\n",
    "                    y_true = [np.nonzero(x)[0][0] for x in dev_tag]\n",
    "                    y_true = y_true[0:len(loss_all)*con.Batch_Size]\n",
    "                    f1 = f1_score(np.array(y_true), np.array(predictions), average='weighted')\n",
    "                    print('分类报告:\\n', metrics.classification_report(np.array(y_true), predictions))\n",
    "                    print(\"验证集：loss {:g}, acc {:g}, f1 {:g}\\n\".format(np.mean(np.array(loss_all)),\n",
    "                                                                      np.mean(np.array(accuracy_all)), f1))\n",
    "                    return f1\n",
    "\n",
    "                model.is_trainning = False\n",
    "                f1 = dev_step()\n",
    "\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    saver.save(sess, \"save_model/ckpt/model.ckpt\")\n",
    "                    print(\"Saved model success\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train = TrainModel()\n",
    "    train.trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from save_model/ckpt\\model.ckpt\n",
      "(array([1], dtype=int64), array([[0.3601215, 0.6398785]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.learn as learn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import data_prepare\n",
    "\n",
    "data_pre = data_prepare.Data_Prepare()\n",
    "\n",
    "\n",
    "class Infer(object):\n",
    "    \"\"\"\n",
    "        ues model to predict classification.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab_processor = learn.preprocessing.VocabularyProcessor.restore('save_model/vocab/vocab.pickle')\n",
    "        self.checkpoint_file = tf.train.latest_checkpoint('save_model/ckpt')\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "            self.sess = tf.Session(config=session_conf)\n",
    "            with self.sess.as_default():\n",
    "                # Load the saved meta graph and restore variables\n",
    "                saver = tf.train.import_meta_graph(\"{}.meta\".format(self.checkpoint_file))\n",
    "                saver.restore(self.sess, self.checkpoint_file)\n",
    "\n",
    "                # Get the placeholders from the graph by name\n",
    "                self.text_a = graph.get_operation_by_name(\"bi_listm_model/text_a\").outputs[0]\n",
    "                self.text_b = graph.get_operation_by_name(\"bi_listm_model/text_b\").outputs[0]\n",
    "                self.a_length = graph.get_operation_by_name(\"bi_listm_model/a_length\").outputs[0]\n",
    "                self.b_length = graph.get_operation_by_name(\"bi_listm_model/b_length\").outputs[0]\n",
    "                self.drop_keep_prob = graph.get_operation_by_name(\"bi_listm_model/dropout_keep_prob\").outputs[0]\n",
    "\n",
    "                # Tensors we want to evaluate\n",
    "                self.prediction = graph.get_operation_by_name(\"bi_listm_model/output/prediction\").outputs[0]\n",
    "                self.score = graph.get_operation_by_name(\"bi_listm_model/output/score\").outputs[0]\n",
    "\n",
    "    def infer(self, sentenceA, sentenceB):\n",
    "        # transfer to vector\n",
    "        sentenceA = [data_pre.pre_processing(sentenceA)]\n",
    "        sentenceB = [data_pre.pre_processing(sentenceB)]\n",
    "        vector_A = np.array(list(self.vocab_processor.transform(sentenceA)))\n",
    "        vector_B = np.array(list(self.vocab_processor.transform(sentenceB)))\n",
    "        feed_dict = {\n",
    "            self.text_a: vector_A,\n",
    "            self.text_b: vector_B,\n",
    "            self.drop_keep_prob: 1.0,\n",
    "            self.a_length: np.array([len(sentenceA[0].split(\" \"))]),\n",
    "            self.b_length: np.array([len(sentenceB[0].split(\" \"))])\n",
    "        }\n",
    "        y, s = self.sess.run([self.prediction, self.score], feed_dict)\n",
    "        return y, s\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    infer = Infer()\n",
    "    sentencea = '你点击详情'\n",
    "    sentenceb = '您点击详情'\n",
    "    print(infer.infer(sentencea, sentenceb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
