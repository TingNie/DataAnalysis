{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import re\n",
    "import jieba\n",
    "import random\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "class Data_Prepare(object):\n",
    "\n",
    "    def readfile(self, filename):\n",
    "        texta = []\n",
    "        textb = []\n",
    "        tag = []\n",
    "        # 读取texta,textb,tag,并且分词\n",
    "        with open(filename, 'r',encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip().split(\"\\t\")\n",
    "                texta.append(self.pre_processing(line[0]))\n",
    "                textb.append(self.pre_processing(line[1]))\n",
    "                tag.append(line[2])\n",
    "        # shuffle\n",
    "        index = [x for x in range(len(texta))]\n",
    "        random.shuffle(index)\n",
    "        texta_new = [texta[x] for x in index]\n",
    "        textb_new = [textb[x] for x in index]\n",
    "        tag_new = [tag[x] for x in index]\n",
    "        # 类别个数，构成[0,1]多分类向量\n",
    "        type = list(set(tag_new))\n",
    "        dicts = {}\n",
    "        tags_vec = []\n",
    "        for x in tag_new:\n",
    "            if x not in dicts.keys():\n",
    "                dicts[x] = 1\n",
    "            else:\n",
    "                dicts[x] += 1\n",
    "            temp = [0] * len(type)\n",
    "            temp[int(x)] = 1\n",
    "            tags_vec.append(temp)\n",
    "        print(dicts)\n",
    "        return texta_new, textb_new, tags_vec\n",
    "\n",
    "    def pre_processing(self, text):\n",
    "        # 删除（）里的内容\n",
    "        text = re.sub('（[^（.]*）', '', text)\n",
    "        # 只保留中文部分\n",
    "        text = ''.join([x for x in text if '\\u4e00' <= x <= '\\u9fa5'])\n",
    "        # 利用jieba进行分词\n",
    "        words = ' '.join(jieba.cut(text)).split(\" \")\n",
    "        # 不分词\n",
    "        words = [x for x in ''.join(words)]\n",
    "        return ' '.join(words)\n",
    "        \n",
    "    def build_vocab(self, sentences, path):\n",
    "        print('start build_vocab...................')\n",
    "        lens = [len(sentence.split(\" \")) for sentence in sentences]\n",
    "        max_length = max(lens)\n",
    "        vocab_processor = learn.preprocessing.VocabularyProcessor(max_length)\n",
    "        vocab_processor.fit(sentences)\n",
    "        vocab_processor.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\nie\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.617 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 79, '0': 45}\n",
      "['如 何 做 生 意 赚 钱 啊', '天 天 酷 跑 怎 么 刷 积 分', '朋 友 过 生 日 唱 什 么 歌', '求 音 乐 世 界 不 是 我 们 的 家', '怎 么 查 通 话 记 录', '求 天 空 之 城 吉 他 谱 六 线 谱 简 单 点', '你 表 情 搞 笑 吗', '有 什 么 电 影 可 以 介 绍 吗', '百 度 上 的 财 富 值 有 什 么 用', '梦 见 抱 小 女 孩']\n"
     ]
    }
   ],
   "source": [
    "data_pre = Data_Prepare()\n",
    "texta_new, textb_new, tags_vec = data_pre.readfile('data/1.txt')\n",
    "print(texta_new[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data_pre.readfile()......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\nie\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.972 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 100192, '1': 138574}\n",
      "start build_vocab...................\n",
      "WARNING:tensorflow:From <ipython-input-1-ceed58ae95bb>:68: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "train_texta_embedding: [[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0]\n",
      " [15 16 17 18 19 20 21 22 23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0]\n",
      " [24 25 26 27 28  6 29 30 31 32 33 34 15 35 36  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0]\n",
      " [10 37 38 39 40 41 42 43 44 45 46 47  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0]\n",
      " [48 49 50 51 52 53 54  5  6 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0]]\n",
      "train_textb_embedding: [[1731  326    5    6  608  935    9   10   11   12   13   14    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0]\n",
      " [  15   16   17   18   19   20   21   22   23    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0]\n",
      " [  24   25   26   27   53    5    6   78  156   90    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0]\n",
      " [  10   37   38   39   40   41   42   43   28    6  278    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0]\n",
      " [1141   49   50   51  395   24   20  466  198 1739   42 1739  957   47\n",
      "  1720 1042    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0]]\n",
      " data_pre.readfile()......................\n",
      "{'0': 4400, '1': 4402}\n",
      "text_a: (?, 35)\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "text_a_embed: (?, 35, 50)\n",
      "WARNING:tensorflow:From <ipython-input-1-ceed58ae95bb>:201: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-1-ceed58ae95bb>:209: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "a_bar: (?, 35, 200)\n",
      "attention_weights: (?, 35, 35)\n",
      "a_hat: (?, 35, 200)\n",
      "m_a: (?, 35, 800)\n",
      "v_a: (?, 35, 200)\n",
      "v_a_avg: (?, 200)\n",
      "v: (?, 800)\n",
      "WARNING:tensorflow:From <ipython-input-1-ceed58ae95bb>:172: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "training 1>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第1次迭代的损失为：12.370276;准确率为：0.52734375\n",
      "验证集：loss 12.0098, acc 0.505\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 201>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第201次迭代的损失为：0.7950944;准确率为：0.67578125\n",
      "training 401>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第401次迭代的损失为：0.63304645;准确率为：0.703125\n",
      "验证集：loss 0.795704, acc 0.5148\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 601>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第601次迭代的损失为：0.6417999;准确率为：0.6796875\n",
      "training 801>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第801次迭代的损失为：0.6554018;准确率为：0.6640625\n",
      "training 1001>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第1001次迭代的损失为：0.643498;准确率为：0.6796875\n",
      "验证集：loss 0.777548, acc 0.5388\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 1201>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第1201次迭代的损失为：0.58530706;准确率为：0.7578125\n",
      "training 1401>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第1401次迭代的损失为：0.5874087;准确率为：0.734375\n",
      "验证集：loss 0.76351, acc 0.5782\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 1601>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第1601次迭代的损失为：0.56306607;准确率为：0.76171875\n",
      "training 1801>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第1801次迭代的损失为：0.64861023;准确率为：0.69140625\n",
      "training 2001>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第2001次迭代的损失为：0.5884;准确率为：0.75\n",
      "验证集：loss 0.759624, acc 0.5844\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 2201>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第2201次迭代的损失为：0.5705902;准确率为：0.76953125\n",
      "training 2401>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第2401次迭代的损失为：0.6208519;准确率为：0.71484375\n",
      "验证集：loss 0.754143, acc 0.596\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 2601>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第2601次迭代的损失为：0.5845015;准确率为：0.76171875\n",
      "training 2801>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第2801次迭代的损失为：0.6089098;准确率为：0.734375\n",
      "training 3001>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第3001次迭代的损失为：0.5502107;准确率为：0.7734375\n",
      "验证集：loss 0.769372, acc 0.599\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 3201>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第3201次迭代的损失为：0.58137095;准确率为：0.75\n",
      "training 3401>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第3401次迭代的损失为：0.55331415;准确率为：0.81640625\n",
      "验证集：loss 0.766381, acc 0.6076\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 3601>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第3601次迭代的损失为：0.6219982;准确率为：0.7265625\n",
      "training 3801>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第3801次迭代的损失为：0.56268084;准确率为：0.79296875\n",
      "training 4001>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第4001次迭代的损失为：0.56108004;准确率为：0.7734375\n",
      "验证集：loss 0.759786, acc 0.6132\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 4201>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第4201次迭代的损失为：0.55394065;准确率为：0.765625\n",
      "training 4401>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第4401次迭代的损失为：0.56477076;准确率为：0.8046875\n",
      "验证集：loss 0.765222, acc 0.6154\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 4601>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第4601次迭代的损失为：0.5959631;准确率为：0.75\n",
      "training 4801>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第4801次迭代的损失为：0.5847237;准确率为：0.765625\n",
      "training 5001>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第5001次迭代的损失为：0.52781564;准确率为：0.796875\n",
      "验证集：loss 0.760149, acc 0.6278\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 5201>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第5201次迭代的损失为：0.60639465;准确率为：0.76953125\n",
      "training 5401>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第5401次迭代的损失为：0.59847754;准确率为：0.79296875\n",
      "验证集：loss 0.751278, acc 0.6214\n",
      "\n",
      "training 5601>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第5601次迭代的损失为：0.5499302;准确率为：0.78125\n",
      "training 5801>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第5801次迭代的损失为：0.5800332;准确率为：0.7734375\n",
      "training 6001>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第6001次迭代的损失为：0.56555355;准确率为：0.7890625\n",
      "验证集：loss 0.756027, acc 0.6186\n",
      "\n",
      "training 6201>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第6201次迭代的损失为：0.556403;准确率为：0.79296875\n",
      "training 6401>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第6401次迭代的损失为：0.5557263;准确率为：0.81640625\n",
      "验证集：loss 0.723888, acc 0.6278\n",
      "\n",
      "training 6601>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第6601次迭代的损失为：0.5345561;准确率为：0.796875\n",
      "training 6801>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第6801次迭代的损失为：0.58072853;准确率为：0.765625\n",
      "training 7001>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第7001次迭代的损失为：0.573788;准确率为：0.7734375\n",
      "验证集：loss 0.735562, acc 0.6322\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 7201>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第7201次迭代的损失为：0.59304106;准确率为：0.75\n",
      "training 7401>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第7401次迭代的损失为：0.58460236;准确率为：0.7578125\n",
      "验证集：loss 0.737262, acc 0.6396\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 7601>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第7601次迭代的损失为：0.5915619;准确率为：0.7734375\n",
      "training 7801>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第7801次迭代的损失为：0.5738025;准确率为：0.7734375\n",
      "training 8001>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第8001次迭代的损失为：0.5322557;准确率为：0.796875\n",
      "验证集：loss 0.753905, acc 0.6286\n",
      "\n",
      "training 8201>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第8201次迭代的损失为：0.6146725;准确率为：0.71484375\n",
      "training 8401>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第8401次迭代的损失为：0.5567451;准确率为：0.78515625\n",
      "验证集：loss 0.735758, acc 0.632\n",
      "\n",
      "training 8601>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第8601次迭代的损失为：0.610635;准确率为：0.7578125\n",
      "training 8801>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第8801次迭代的损失为：0.58528256;准确率为：0.7578125\n",
      "training 9001>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第9001次迭代的损失为：0.5483494;准确率为：0.79296875\n",
      "验证集：loss 0.73983, acc 0.6406\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 9201>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第9201次迭代的损失为：0.54154676;准确率为：0.8203125\n",
      "training 9401>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第9401次迭代的损失为：0.57679665;准确率为：0.78125\n",
      "验证集：loss 0.731004, acc 0.6478\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 9601>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第9601次迭代的损失为：0.5406002;准确率为：0.8125\n",
      "training 9801>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "第9801次迭代的损失为：0.54584;准确率为：0.796875\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3' \n",
    "\n",
    "import re\n",
    "import jieba\n",
    "import random\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "class Data_Prepare(object):\n",
    "\n",
    "    def readfile(self, filename):\n",
    "        texta = []\n",
    "        textb = []\n",
    "        tag = []\n",
    "        # 读取texta,textb,tag,并且分词\n",
    "        with open(filename, 'r',encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip().split(\"\\t\")\n",
    "                texta.append(self.pre_processing(line[0]))\n",
    "                textb.append(self.pre_processing(line[1]))\n",
    "                tag.append(line[2])\n",
    "        # shuffle\n",
    "        index = [x for x in range(len(texta))]\n",
    "        random.shuffle(index)\n",
    "        texta_new = [texta[x] for x in index]\n",
    "        textb_new = [textb[x] for x in index]\n",
    "        tag_new = [tag[x] for x in index]\n",
    "        # 类别个数，构成[0,1]多分类向量\n",
    "        type = list(set(tag_new))\n",
    "        dicts = {}\n",
    "        tags_vec = []\n",
    "        for x in tag_new:\n",
    "            if x not in dicts.keys():\n",
    "                dicts[x] = 1\n",
    "            else:\n",
    "                dicts[x] += 1\n",
    "            temp = [0] * len(type)\n",
    "            temp[int(x)] = 1\n",
    "            tags_vec.append(temp)\n",
    "        print(dicts)\n",
    "        return texta_new, textb_new, tags_vec\n",
    "\n",
    "    def pre_processing(self, text):\n",
    "        # 删除（）里的内容\n",
    "        text = re.sub('（[^（.]*）', '', text)\n",
    "        # 只保留中文部分\n",
    "        text = ''.join([x for x in text if '\\u4e00' <= x <= '\\u9fa5'])\n",
    "        # 利用jieba进行分词\n",
    "        words = ' '.join(jieba.cut(text)).split(\" \")\n",
    "        # 不分词\n",
    "        words = [x for x in ''.join(words)]\n",
    "        return ' '.join(words)\n",
    "        \n",
    "    def build_vocab(self, sentences, path):\n",
    "        print('start build_vocab...................')\n",
    "        lens = [len(sentence.split(\" \")) for sentence in sentences]\n",
    "        max_length = max(lens)\n",
    "        vocab_processor = learn.preprocessing.VocabularyProcessor(max_length)\n",
    "        vocab_processor.fit(sentences)\n",
    "        vocab_processor.save(path)\n",
    "\n",
    "        \n",
    "class Config(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.embedding_size = 50  # 词向量维度\n",
    "        self.hidden_num = 100  # 隐藏层规模\n",
    "        self.l2_lambda = 0.01\n",
    "        self.learning_rate = 0.001\n",
    "        self.dropout_keep_prob = 0.5\n",
    "        \n",
    "\n",
    "        self.epoch = 10000\n",
    "        self.Batch_Size = 256\n",
    "        \n",
    "\n",
    "class ESIM(object):\n",
    "\n",
    "    def __init__(self, is_trainning, seq_length, class_num, vocabulary_size, embedding_size, hidden_num,\n",
    "                 l2_lambda, learning_rate):\n",
    "        self.is_trainning = is_trainning\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_num = hidden_num\n",
    "        self.seq_length = seq_length\n",
    "        self.class_num = class_num\n",
    "\n",
    "        # init placeholder\n",
    "        self.text_a = tf.placeholder(tf.int32, [None, seq_length], name='text_a')\n",
    "        self.text_b = tf.placeholder(tf.int32, [None, seq_length], name='text_b')\n",
    "        self.y = tf.placeholder(tf.int32, [None, class_num], name='y')\n",
    "        print('text_a:',self.text_a.shape)\n",
    "        \n",
    "        # real length\n",
    "        self.a_length = tf.placeholder(tf.int32, [None], name='a_length')\n",
    "        self.b_length = tf.placeholder(tf.int32, [None], name='b_length')\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # embedding层 论文中采用是预训练好的词向量 这里随机初始化一个词典 在训练过程中进行调整\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.vocab_matrix = tf.Variable(tf.truncated_normal(shape=[vocabulary_size, embedding_size],\n",
    "                                                                stddev=1.0 / math.sqrt(embedding_size)),\n",
    "                                            name='vacab_matrix')\n",
    "            self.text_a_embed = tf.nn.embedding_lookup(self.vocab_matrix, self.text_a)\n",
    "            self.text_b_embed = tf.nn.embedding_lookup(self.vocab_matrix, self.text_b)\n",
    "            \n",
    "            print('text_a_embed:',self.text_a_embed.shape)\n",
    "            \n",
    "        # Input Encoding\n",
    "        with tf.name_scope('Input_Encoding'):\n",
    "            a_bar = self.biLSTMBlock(self.text_a_embed, hidden_num, 'Input_Encoding/biLSTM', self.a_length)\n",
    "            b_bar = self.biLSTMBlock(self.text_b_embed, hidden_num, 'Input_Encoding/biLSTM', self.b_length, isreuse=True)\n",
    "            print('a_bar:',a_bar.shape)\n",
    "        # Local Inference Modeling\n",
    "        with tf.name_scope('Local_inference_Modeling'):\n",
    "            # 计算a_bar与b_bar每个词语之间的相似度\n",
    "            with tf.name_scope('word_similarity'):\n",
    "                attention_weights = tf.matmul(a_bar, tf.transpose(b_bar, [0, 2, 1]))\n",
    "                print('attention_weights:',attention_weights.shape)\n",
    "                \n",
    "                attentionsoft_a = tf.nn.softmax(attention_weights)\n",
    "                attentionsoft_b = tf.nn.softmax(tf.transpose(attention_weights, [0, 2, 1]))\n",
    "                #attentionsoft_b = tf.transpose(attentionsoft_b)\n",
    "                a_hat = tf.matmul(attentionsoft_a, b_bar)\n",
    "                b_hat = tf.matmul(attentionsoft_b, a_bar)\n",
    "                print('a_hat:',a_hat.shape)\n",
    "\n",
    "            # 计算m_a, m_b\n",
    "            with tf.name_scope(\"compute_m_a/m_b\"):\n",
    "                a_diff = tf.subtract(a_bar, a_hat)\n",
    "                a_mul = tf.multiply(a_bar, a_hat)\n",
    "\n",
    "                b_diff = tf.subtract(b_bar, b_hat)\n",
    "                b_mul = tf.multiply(b_bar, b_hat)\n",
    "\n",
    "                # m_a = [a_bar, a_hat, a_bar - a_hat, a_bar 'dot' a_hat] (14)\n",
    "                # m_b = [b_bar, b_hat, b_bar - b_hat, b_bar 'dot' b_hat] (15)\n",
    "                self.m_a = tf.concat([a_bar, a_hat, a_diff, a_mul], axis=2)\n",
    "                self.m_b = tf.concat([b_bar, b_hat, b_diff, b_mul], axis=2)\n",
    "                \n",
    "                print('m_a:',self.m_a.shape)\n",
    "\n",
    "        with tf.name_scope(\"Inference_Composition\"):\n",
    "            v_a = self.biLSTMBlock(self.m_a, hidden_num, 'Inference_Composition/biLSTM', self.a_length)\n",
    "            v_b = self.biLSTMBlock(self.m_b, hidden_num, 'Inference_Composition/biLSTM', self.b_length, isreuse=True)\n",
    "            print('v_a:',v_a.shape)\n",
    "            # average pool and max pool\n",
    "            v_a_avg = tf.reduce_mean(v_a, axis=1)\n",
    "            v_b_avg = tf.reduce_mean(v_b, axis=1)\n",
    "            v_a_max = tf.reduce_max(v_a, axis=1)\n",
    "            v_b_max = tf.reduce_max(v_b, axis=1)\n",
    "            \n",
    "            print('v_a_avg:',v_a_avg.shape)\n",
    "\n",
    "            v = tf.concat([v_a_avg, v_a_max, v_b_avg, v_b_max], axis=1)\n",
    "            print('v:',v.shape)\n",
    "            \n",
    "        with tf.name_scope(\"output\"):\n",
    "            initializer = tf.random_normal_initializer(0.0, 0.1)\n",
    "            with tf.variable_scope('feed_foward_layer1'):\n",
    "                inputs = tf.nn.dropout(v, self.dropout_keep_prob)\n",
    "                outputs = tf.layers.dense(inputs, hidden_num, tf.nn.relu, kernel_initializer=initializer)\n",
    "            with tf.variable_scope('feed_foward_layer2'):\n",
    "                outputs = tf.nn.dropout(outputs, self.dropout_keep_prob)\n",
    "                self.logits = tf.layers.dense(outputs, class_num, tf.nn.tanh, kernel_initializer=initializer)\n",
    "            self.score = tf.nn.softmax(self.logits, name='score')\n",
    "            self.prediction = tf.argmax(self.score, 1, name=\"prediction\")\n",
    "\n",
    "        with tf.name_scope('cost'):\n",
    "            self.cost = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.y, logits=self.logits)\n",
    "            self.cost = tf.reduce_mean(self.cost)\n",
    "            # l2正则化\n",
    "            weights = [v for v in tf.trainable_variables() if ('w' in v.name) or ('kernel' in v.name)]\n",
    "            l2_loss = tf.add_n([tf.nn.l2_loss(w) for w in weights]) * l2_lambda\n",
    "            self.loss = l2_loss + self.cost\n",
    "\n",
    "        self.accuracy = tf.reduce_mean(\n",
    "            tf.cast(tf.equal(tf.argmax(self.y, axis=1), self.prediction), tf.float32))\n",
    "\n",
    "        if not is_trainning:\n",
    "            return\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), 5)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    def biLSTMBlock(self, inputs, num_units, scope, seq_len=None, isreuse=False):\n",
    "        with tf.variable_scope(scope, reuse=isreuse):\n",
    "            cell_fw = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "            fw_lstm_cell = tf.contrib.rnn.DropoutWrapper(cell_fw, output_keep_prob=self.dropout_keep_prob)\n",
    "            cell_bw = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "            bw_lstm_cell = tf.contrib.rnn.DropoutWrapper(cell_bw, output_keep_prob=self.dropout_keep_prob)\n",
    "\n",
    "            (a_outputs, a_output_states) = tf.nn.bidirectional_dynamic_rnn(fw_lstm_cell, bw_lstm_cell,\n",
    "                                                                           inputs,\n",
    "                                                                           sequence_length=seq_len,\n",
    "                                                                           dtype=tf.float32)\n",
    "            a_bar = tf.concat(a_outputs, axis=2)\n",
    "            return a_bar\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "con = Config()\n",
    "data_pre = Data_Prepare()\n",
    "\n",
    "\n",
    "class TrainModel(object):\n",
    "    '''\n",
    "        训练模型\n",
    "        保存模型\n",
    "    '''\n",
    "    def pre_processing(self):\n",
    "        print(' data_pre.readfile()......................')\n",
    "        train_texta, train_textb, train_tag = data_pre.readfile('data/train.txt')\n",
    "        data = []\n",
    "        data.extend(train_texta)\n",
    "        data.extend(train_textb)\n",
    "        #训练集构造词典，并且保存\n",
    "        data_pre.build_vocab(data, 'save_model/vocab/vocab.pickle')\n",
    "        # 加载词典\n",
    "        self.vocab_processor = learn.preprocessing.VocabularyProcessor.restore('save_model/vocab/vocab.pickle')\n",
    "        # 构成id\n",
    "        train_texta_embedding = np.array(list(self.vocab_processor.transform(train_texta)))\n",
    "        train_textb_embedding = np.array(list(self.vocab_processor.transform(train_textb)))\n",
    "        print('train_texta_embedding:',train_texta_embedding[:5])\n",
    "        print('train_textb_embedding:',train_textb_embedding[:5])\n",
    "        \n",
    "        print(' data_pre.readfile()......................')\n",
    "        dev_texta, dev_textb, dev_tag = data_pre.readfile('data/dev.txt')\n",
    "        dev_texta_embedding = np.array(list(self.vocab_processor.transform(dev_texta)))\n",
    "        dev_textb_embedding = np.array(list(self.vocab_processor.transform(dev_textb)))\n",
    "        return train_texta_embedding, train_textb_embedding, np.array(train_tag), \\\n",
    "               dev_texta_embedding, dev_textb_embedding, np.array(dev_tag)\n",
    "\n",
    "    def get_batches(self, texta, textb, tag, size):\n",
    "        '''num_batch = int(len(texta) / con.Batch_Size)\n",
    "        for i in range(num_batch):\n",
    "            a = texta[i*con.Batch_Size:(i+1)*con.Batch_Size]\n",
    "            b = textb[i*con.Batch_Size:(i+1)*con.Batch_Size]\n",
    "            t = tag[i*con.Batch_Size:(i+1)*con.Batch_Size]\n",
    "            yield a, b, t\n",
    "        '''\n",
    "        index = np.random.choice(len(texta),size,replace=True)\n",
    "        a = texta[index]\n",
    "        b = textb[index]\n",
    "        t = tag[index]\n",
    "        return a,b,t\n",
    "        \n",
    "    def get_length(self, trainX_batch):\n",
    "        # sentence length\n",
    "        lengths = []\n",
    "        for sample in trainX_batch:\n",
    "            count = 0\n",
    "            for index in sample:\n",
    "                if index != 0:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "            lengths.append(count)\n",
    "        return lengths\n",
    "\n",
    "    def trainModel(self):\n",
    "        train_texta_embedding, train_textb_embedding, train_tag, \\\n",
    "        dev_texta_embedding, dev_textb_embedding, dev_tag = self.pre_processing()\n",
    "        # 定义训练用的循环神经网络模型\n",
    "        with tf.variable_scope('esim_model', reuse=None):\n",
    "            # esim model ， 按照seq_length = 第一个的长度\n",
    "            model = ESIM(True, seq_length=len(train_texta_embedding[0]),\n",
    "                                    class_num=len(train_tag[0]),\n",
    "                                    vocabulary_size=len(self.vocab_processor.vocabulary_),\n",
    "                                    embedding_size=con.embedding_size,\n",
    "                                    hidden_num=con.hidden_num,\n",
    "                                    l2_lambda=con.l2_lambda,\n",
    "                                    learning_rate=con.learning_rate)\n",
    "\n",
    "        # 训练模型\n",
    "        with tf.Session() as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            saver = tf.train.Saver()\n",
    "            best_f1 = 0.0\n",
    "            for time in range(con.epoch):\n",
    "                \n",
    "                model.is_trainning = True\n",
    "                \n",
    "                texta,textb,tag = self.get_batches(train_texta_embedding, train_textb_embedding, train_tag , con.Batch_Size)\n",
    "                feed_dict = {\n",
    "                    model.text_a: texta,\n",
    "                    model.text_b: textb,\n",
    "                    model.y: tag,\n",
    "                    model.dropout_keep_prob: con.dropout_keep_prob,\n",
    "                    model.a_length: np.array(self.get_length(texta)),\n",
    "                    model.b_length: np.array(self.get_length(textb))\n",
    "                }\n",
    "                _, cost, accuracy = sess.run([model.train_op, model.loss, model.accuracy], feed_dict)\n",
    "               \n",
    "                if time%200==0:\n",
    "                    print(\"training \" + str(time + 1) + \">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "                    \n",
    "                    print(\"第\" + str((time + 1)) + \"次迭代的损失为：\" + str(cost) + \";准确率为：\" +str(accuracy))\n",
    "\n",
    "                def dev_step():\n",
    "                    \"\"\"\n",
    "                    Evaluates model on a dev set\n",
    "                    \"\"\"\n",
    "                    texta,textb,tag = self.get_batches(dev_texta_embedding, dev_textb_embedding, dev_tag,size = 5000)\n",
    "                    feed_dict = {\n",
    "                        model.text_a: texta,\n",
    "                        model.text_b: textb,\n",
    "                        model.y: tag,\n",
    "                        model.dropout_keep_prob: con.dropout_keep_prob,\n",
    "                        model.a_length: np.array(self.get_length(texta)),\n",
    "                        model.b_length: np.array(self.get_length(textb))\n",
    "                    }\n",
    "                    # 不训练\n",
    "                    dev_cost, dev_accuracy, prediction = sess.run([model.loss, model.accuracy,\n",
    "                                                                   model.prediction], feed_dict)\n",
    "                   \n",
    "                    print(\"验证集：loss {:g}, acc {:g}\\n\".format(dev_cost,dev_accuracy))\n",
    "                    return dev_accuracy\n",
    "\n",
    "                if time%500==0:\n",
    "                    model.is_trainning = False\n",
    "                    f1 = dev_step()\n",
    "\n",
    "                    if f1 > best_f1:\n",
    "                        best_f1 = f1\n",
    "                        saver.save(sess, \"save_model/ckpt/model.ckpt\")\n",
    "                        print(\"Saved model success\\n\")\n",
    "\n",
    "\n",
    "\n",
    "train = TrainModel()\n",
    "train.trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\nie\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.627 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 138574, '0': 100192}\n",
      "start build_vocab...................\n",
      "WARNING:tensorflow:From <ipython-input-1-a86c594c81ac>:68: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "train_texta_embedding: [[ 1  2  3  4  5  6  7  8  9 10 11  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0]\n",
      " [12 13 14 15 16 17 18  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0]\n",
      " [19 20 16 17 21 10 22  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 5  6  7  8  9 23 24 25 26 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0]\n",
      " [27 28 29 30 31 32 33 34 35 36 37  9 38 39  4 40 17  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0]]\n",
      "train_textb_embedding: [[   5    6    7    8    1    2    3   10    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0]\n",
      " [  97   12   13   16   17   18    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0]\n",
      " [  16   18 1328   19   20  670   10   22    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0]\n",
      " [   5    6    7    8    9   23   24   25   26   10    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0]\n",
      " [  63   32   33  464   30    5    6  463  692  486  225  107   85  357\n",
      "   311   32   33   36   37    9   38   39    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0]]\n",
      "{'0': 4400, '1': 4402}\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From E:\\python_workSpace\\text_match\\ESIM\\esim_model.py:109: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From E:\\python_workSpace\\text_match\\ESIM\\esim_model.py:117: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From E:\\python_workSpace\\text_match\\ESIM\\esim_model.py:81: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "training 1>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [28:26,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次迭代的损失为：0.72860974;准确率为：0.6861822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:22,  8.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.37      0.46      4398\n",
      "           1       0.55      0.78      0.65      4402\n",
      "\n",
      "   micro avg       0.57      0.57      0.57      8800\n",
      "   macro avg       0.59      0.57      0.55      8800\n",
      "weighted avg       0.59      0.57      0.55      8800\n",
      "\n",
      "验证集：loss 0.738797, acc 0.572955, f1 0.554089\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 2>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [31:47,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2次迭代的损失为：0.60668314;准确率为：0.7428104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:23,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.41      0.51      4398\n",
      "           1       0.58      0.80      0.67      4402\n",
      "\n",
      "   micro avg       0.61      0.61      0.61      8800\n",
      "   macro avg       0.62      0.61      0.59      8800\n",
      "weighted avg       0.62      0.61      0.59      8800\n",
      "\n",
      "验证集：loss 0.743938, acc 0.605227, f1 0.589432\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 3>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [32:01,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第3次迭代的损失为：0.59090793;准确率为：0.76292354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:17,  9.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.46      0.55      4398\n",
      "           1       0.59      0.79      0.68      4402\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      8800\n",
      "   macro avg       0.64      0.63      0.62      8800\n",
      "weighted avg       0.64      0.63      0.62      8800\n",
      "\n",
      "验证集：loss 0.735546, acc 0.626023, f1 0.615332\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 4>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [29:54,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第4次迭代的损失为：0.58163154;准确率为：0.77341574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:18,  9.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.43      0.53      4398\n",
      "           1       0.59      0.81      0.68      4402\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      8800\n",
      "   macro avg       0.64      0.62      0.61      8800\n",
      "weighted avg       0.64      0.62      0.61      8800\n",
      "\n",
      "验证集：loss 0.736841, acc 0.621591, f1 0.606992\n",
      "\n",
      "training 5>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [30:01,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第5次迭代的损失为：0.57436895;准确率为：0.78097594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:18,  9.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.45      0.55      4398\n",
      "           1       0.59      0.81      0.69      4402\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      8800\n",
      "   macro avg       0.65      0.63      0.62      8800\n",
      "weighted avg       0.65      0.63      0.62      8800\n",
      "\n",
      "验证集：loss 0.733036, acc 0.628182, f1 0.61537\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 6>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [29:53,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第6次迭代的损失为：0.56883144;准确率为：0.78665966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:18,  9.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.49      0.57      4398\n",
      "           1       0.60      0.77      0.68      4402\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      8800\n",
      "   macro avg       0.64      0.63      0.62      8800\n",
      "weighted avg       0.64      0.63      0.62      8800\n",
      "\n",
      "验证集：loss 0.75233, acc 0.629205, f1 0.621481\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 7>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [30:06,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第7次迭代的损失为：0.5632268;准确率为：0.79365027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:17,  9.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.48      0.57      4398\n",
      "           1       0.61      0.79      0.69      4402\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      8800\n",
      "   macro avg       0.65      0.64      0.63      8800\n",
      "weighted avg       0.65      0.64      0.63      8800\n",
      "\n",
      "验证集：loss 0.745146, acc 0.638068, f1 0.629118\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 8>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [22:31,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第8次迭代的损失为：0.5584282;准确率为：0.7985592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:10, 17.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.46      0.56      4398\n",
      "           1       0.60      0.82      0.69      4402\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      8800\n",
      "   macro avg       0.66      0.64      0.63      8800\n",
      "weighted avg       0.66      0.64      0.63      8800\n",
      "\n",
      "验证集：loss 0.742634, acc 0.637841, f1 0.625918\n",
      "\n",
      "training 9>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [18:22,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第9次迭代的损失为：0.5558104;准确率为：0.80203146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:10, 17.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.49      0.57      4398\n",
      "           1       0.61      0.79      0.69      4402\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      8800\n",
      "   macro avg       0.65      0.64      0.63      8800\n",
      "weighted avg       0.65      0.64      0.63      8800\n",
      "\n",
      "验证集：loss 0.750597, acc 0.638182, f1 0.629645\n",
      "\n",
      "Saved model success\n",
      "\n",
      "training 10>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4775it [18:20,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第10次迭代的损失为：0.549713;准确率为：0.8074681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "176it [00:10, 16.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.45      0.55      4398\n",
      "           1       0.59      0.79      0.68      4402\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      8800\n",
      "   macro avg       0.64      0.62      0.61      8800\n",
      "weighted avg       0.64      0.62      0.61      8800\n",
      "\n",
      "验证集：loss 0.768634, acc 0.623068, f1 0.611865\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3' \n",
    "\n",
    "import re\n",
    "import jieba\n",
    "import random\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "class Data_Prepare(object):\n",
    "\n",
    "    def readfile(self, filename):\n",
    "        texta = []\n",
    "        textb = []\n",
    "        tag = []\n",
    "        # 读取texta,textb,tag,并且分词\n",
    "        with open(filename, 'r',encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip().split(\"\\t\")\n",
    "                texta.append(self.pre_processing(line[0]))\n",
    "                textb.append(self.pre_processing(line[1]))\n",
    "                tag.append(line[2])\n",
    "        # shuffle\n",
    "        index = [x for x in range(len(texta))]\n",
    "        random.shuffle(index)\n",
    "        texta_new = [texta[x] for x in index]\n",
    "        textb_new = [textb[x] for x in index]\n",
    "        tag_new = [tag[x] for x in index]\n",
    "        # 类别个数，构成[0,1]多分类向量\n",
    "        type = list(set(tag_new))\n",
    "        dicts = {}\n",
    "        tags_vec = []\n",
    "        for x in tag_new:\n",
    "            if x not in dicts.keys():\n",
    "                dicts[x] = 1\n",
    "            else:\n",
    "                dicts[x] += 1\n",
    "            temp = [0] * len(type)\n",
    "            temp[int(x)] = 1\n",
    "            tags_vec.append(temp)\n",
    "        print(dicts)\n",
    "        return texta_new, textb_new, tags_vec\n",
    "\n",
    "    def pre_processing(self, text):\n",
    "        # 删除（）里的内容\n",
    "        text = re.sub('（[^（.]*）', '', text)\n",
    "        # 只保留中文部分\n",
    "        text = ''.join([x for x in text if '\\u4e00' <= x <= '\\u9fa5'])\n",
    "        # 利用jieba进行分词\n",
    "        words = ' '.join(jieba.cut(text)).split(\" \")\n",
    "        # 不分词\n",
    "        words = [x for x in ''.join(words)]\n",
    "        return ' '.join(words)\n",
    "        \n",
    "    def build_vocab(self, sentences, path):\n",
    "        print('start build_vocab...................')\n",
    "        lens = [len(sentence.split(\" \")) for sentence in sentences]\n",
    "        max_length = max(lens)\n",
    "        vocab_processor = learn.preprocessing.VocabularyProcessor(max_length)\n",
    "        vocab_processor.fit(sentences)\n",
    "        vocab_processor.save(path)\n",
    "\n",
    "        \n",
    "class Config(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.embedding_size = 50  # 词向量维度\n",
    "        self.hidden_num = 100  # 隐藏层规模\n",
    "        self.l2_lambda = 0.01\n",
    "        self.learning_rate = 0.001\n",
    "        self.dropout_keep_prob = 0.5\n",
    "        \n",
    "\n",
    "        self.epoch = 10\n",
    "        self.Batch_Size = 50\n",
    "        \n",
    "\n",
    "class ESIM(object):\n",
    "\n",
    "    def __init__(self, is_trainning, seq_length, class_num, vocabulary_size, embedding_size, hidden_num,\n",
    "                 l2_lambda, learning_rate):\n",
    "        self.is_trainning = is_trainning\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_num = hidden_num\n",
    "        self.seq_length = seq_length\n",
    "        self.class_num = class_num\n",
    "\n",
    "        # init placeholder\n",
    "        self.text_a = tf.placeholder(tf.int32, [None, seq_length], name='text_a')\n",
    "        self.text_b = tf.placeholder(tf.int32, [None, seq_length], name='text_b')\n",
    "        self.y = tf.placeholder(tf.int32, [None, class_num], name='y')\n",
    "        print('text_a:',self.text_a.shape)\n",
    "        \n",
    "        # real length\n",
    "        self.a_length = tf.placeholder(tf.int32, [None], name='a_length')\n",
    "        self.b_length = tf.placeholder(tf.int32, [None], name='b_length')\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # embedding层 论文中采用是预训练好的词向量 这里随机初始化一个词典 在训练过程中进行调整\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.vocab_matrix = tf.Variable(tf.truncated_normal(shape=[vocabulary_size, embedding_size],\n",
    "                                                                stddev=1.0 / math.sqrt(embedding_size)),\n",
    "                                            name='vacab_matrix')\n",
    "            self.text_a_embed = tf.nn.embedding_lookup(self.vocab_matrix, self.text_a)\n",
    "            self.text_b_embed = tf.nn.embedding_lookup(self.vocab_matrix, self.text_b)\n",
    "            \n",
    "            print('text_a_embed:',self.text_a_embed.shape)\n",
    "            \n",
    "        # Input Encoding\n",
    "        with tf.name_scope('Input_Encoding'):\n",
    "            a_bar = self.biLSTMBlock(self.text_a_embed, hidden_num, 'Input_Encoding/biLSTM', self.a_length)\n",
    "            b_bar = self.biLSTMBlock(self.text_b_embed, hidden_num, 'Input_Encoding/biLSTM', self.b_length, isreuse=True)\n",
    "            print('a_bar:',a_bar.shape)\n",
    "        # Local Inference Modeling\n",
    "        with tf.name_scope('Local_inference_Modeling'):\n",
    "            # 计算a_bar与b_bar每个词语之间的相似度\n",
    "            with tf.name_scope('word_similarity'):\n",
    "                attention_weights = tf.matmul(a_bar, tf.transpose(b_bar, [0, 2, 1]))\n",
    "                print('attention_weights:',attention_weights.shape)\n",
    "                \n",
    "                attentionsoft_a = tf.nn.softmax(attention_weights)\n",
    "                attentionsoft_b = tf.nn.softmax(tf.transpose(attention_weights,[0, 2, 1]))\n",
    "                #attentionsoft_b = tf.transpose(attentionsoft_b)\n",
    "                a_hat = tf.matmul(attentionsoft_a, b_bar)\n",
    "                b_hat = tf.matmul(attentionsoft_b, a_bar)\n",
    "                print('a_hat:',a_hat.shape)\n",
    "\n",
    "            # 计算m_a, m_b\n",
    "            with tf.name_scope(\"compute_m_a/m_b\"):\n",
    "                a_diff = tf.subtract(a_bar, a_hat)\n",
    "                a_mul = tf.multiply(a_bar, a_hat)\n",
    "\n",
    "                b_diff = tf.subtract(b_bar, b_hat)\n",
    "                b_mul = tf.multiply(b_bar, b_hat)\n",
    "\n",
    "                # m_a = [a_bar, a_hat, a_bar - a_hat, a_bar 'dot' a_hat] (14)\n",
    "                # m_b = [b_bar, b_hat, b_bar - b_hat, b_bar 'dot' b_hat] (15)\n",
    "                self.m_a = tf.concat([a_bar, a_hat, a_diff, a_mul], axis=2)\n",
    "                self.m_b = tf.concat([b_bar, b_hat, b_diff, b_mul], axis=2)\n",
    "                \n",
    "                print('m_a:',self.m_a.shape)\n",
    "\n",
    "        with tf.name_scope(\"Inference_Composition\"):\n",
    "            v_a = self.biLSTMBlock(self.m_a, hidden_num, 'Inference_Composition/biLSTM', self.a_length)\n",
    "            v_b = self.biLSTMBlock(self.m_b, hidden_num, 'Inference_Composition/biLSTM', self.b_length, isreuse=True)\n",
    "            print('v_a:',v_a.shape)\n",
    "            # average pool and max pool\n",
    "            v_a_avg = tf.reduce_mean(v_a, axis=1)\n",
    "            v_b_avg = tf.reduce_mean(v_b, axis=1)\n",
    "            v_a_max = tf.reduce_max(v_a, axis=1)\n",
    "            v_b_max = tf.reduce_max(v_b, axis=1)\n",
    "            \n",
    "            print('v_a_avg:',v_a_avg.shape)\n",
    "\n",
    "            v = tf.concat([v_a_avg, v_a_max, v_b_avg, v_b_max], axis=1)\n",
    "            print('v:',v.shape)\n",
    "            \n",
    "        with tf.name_scope(\"output\"):\n",
    "            initializer = tf.random_normal_initializer(0.0, 0.1)\n",
    "            with tf.variable_scope('feed_foward_layer1'):\n",
    "                inputs = tf.nn.dropout(v, self.dropout_keep_prob)\n",
    "                outputs = tf.layers.dense(inputs, hidden_num, tf.nn.relu, kernel_initializer=initializer)\n",
    "            with tf.variable_scope('feed_foward_layer2'):\n",
    "                outputs = tf.nn.dropout(outputs, self.dropout_keep_prob)\n",
    "                self.logits = tf.layers.dense(outputs, class_num, tf.nn.tanh, kernel_initializer=initializer)\n",
    "            self.score = tf.nn.softmax(self.logits, name='score')\n",
    "            self.prediction = tf.argmax(self.score, 1, name=\"prediction\")\n",
    "\n",
    "        with tf.name_scope('cost'):\n",
    "            self.cost = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.y, logits=self.logits)\n",
    "            self.cost = tf.reduce_mean(self.cost)\n",
    "            # l2正则化\n",
    "            weights = [v for v in tf.trainable_variables() if ('w' in v.name) or ('kernel' in v.name)]\n",
    "            l2_loss = tf.add_n([tf.nn.l2_loss(w) for w in weights]) * l2_lambda\n",
    "            self.loss = l2_loss + self.cost\n",
    "\n",
    "        self.accuracy = tf.reduce_mean(\n",
    "            tf.cast(tf.equal(tf.argmax(self.y, axis=1), self.prediction), tf.float32))\n",
    "\n",
    "        if not is_trainning:\n",
    "            return\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), 5)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    def biLSTMBlock(self, inputs, num_units, scope, seq_len=None, isreuse=False):\n",
    "        with tf.variable_scope(scope, reuse=isreuse):\n",
    "            cell_fw = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "            fw_lstm_cell = tf.contrib.rnn.DropoutWrapper(cell_fw, output_keep_prob=self.dropout_keep_prob)\n",
    "            cell_bw = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "            bw_lstm_cell = tf.contrib.rnn.DropoutWrapper(cell_bw, output_keep_prob=self.dropout_keep_prob)\n",
    "\n",
    "            (a_outputs, a_output_states) = tf.nn.bidirectional_dynamic_rnn(fw_lstm_cell, bw_lstm_cell,\n",
    "                                                                           inputs,\n",
    "                                                                           sequence_length=seq_len,\n",
    "                                                                           dtype=tf.float32)\n",
    "            a_bar = tf.concat(a_outputs, axis=2)\n",
    "            return a_bar\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -*- coding: UTF-8 -*-\n",
    "import tensorflow as tf\n",
    "import data_prepare\n",
    "from tensorflow.contrib import learn\n",
    "import numpy as np\n",
    "import esim_model\n",
    "import config as config\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3' # 屏蔽warning\n",
    "\n",
    "con = Config()\n",
    "data_pre = Data_Prepare()\n",
    "\n",
    "\n",
    "class TrainModel(object):\n",
    "    '''\n",
    "        训练模型\n",
    "        保存模型\n",
    "    '''\n",
    "\n",
    "    def pre_processing(self):\n",
    "\n",
    "        train_texta, train_textb, train_tag = data_pre.readfile('data/train.txt')\n",
    "        data = []\n",
    "        data.extend(train_texta)\n",
    "        data.extend(train_textb)\n",
    "        # 训练集构造词典，并且保存\n",
    "        data_pre.build_vocab(data, 'save_model/vocab/vocab.pickle')\n",
    "        # 加载词典\n",
    "        self.vocab_processor = learn.preprocessing.VocabularyProcessor.restore('save_model/vocab/vocab.pickle')\n",
    "        # 中文句子，转化为id序列，固定长度\n",
    "        train_texta_embedding = np.array(list(self.vocab_processor.transform(train_texta)))\n",
    "        train_textb_embedding = np.array(list(self.vocab_processor.transform(train_textb)))\n",
    "        print('train_texta_embedding:', train_texta_embedding[:5])\n",
    "        print('train_textb_embedding:', train_textb_embedding[:5])\n",
    "\n",
    "\n",
    "        dev_texta, dev_textb, dev_tag = data_pre.readfile('data/dev.txt')\n",
    "        dev_texta_embedding = np.array(list(self.vocab_processor.transform(dev_texta)))\n",
    "        dev_textb_embedding = np.array(list(self.vocab_processor.transform(dev_textb)))\n",
    "        return train_texta_embedding, train_textb_embedding, np.array(train_tag), \\\n",
    "               dev_texta_embedding, dev_textb_embedding, np.array(dev_tag)\n",
    "    # 1轮迭代进行batch循环，训练所有样本\n",
    "    def get_batches(self, texta, textb, tag):\n",
    "        num_batch = int(len(texta) / con.Batch_Size)\n",
    "        for i in range(num_batch):\n",
    "            a = texta[i*con.Batch_Size:(i+1)*con.Batch_Size]\n",
    "            b = textb[i*con.Batch_Size:(i+1)*con.Batch_Size]\n",
    "            t = tag[i*con.Batch_Size:(i+1)*con.Batch_Size]\n",
    "            yield a, b, t\n",
    "    # 获得句子的长度 = [句子1长度，句子2长度.....]\n",
    "    def get_length(self, trainX_batch):\n",
    "        # sentence length\n",
    "        lengths = []\n",
    "        for sample in trainX_batch:\n",
    "            count = 0\n",
    "            for index in sample:\n",
    "                if index != 0:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "            lengths.append(count)\n",
    "        return lengths\n",
    "\n",
    "    def trainModel(self):\n",
    "        train_texta_embedding, train_textb_embedding, train_tag, \\\n",
    "        dev_texta_embedding, dev_textb_embedding, dev_tag = self.pre_processing()\n",
    "        # 定义训练用的循环神经网络模型\n",
    "        with tf.variable_scope('esim_model', reuse=None):\n",
    "            # esim model\n",
    "            model = esim_model.ESIM(True, seq_length=len(train_texta_embedding[0]),\n",
    "                                    class_num=len(train_tag[0]),\n",
    "                                    vocabulary_size=len(self.vocab_processor.vocabulary_),\n",
    "                                    embedding_size=con.embedding_size,\n",
    "                                    hidden_num=con.hidden_num,\n",
    "                                    l2_lambda=con.l2_lambda,\n",
    "                                    learning_rate=con.learning_rate)\n",
    "\n",
    "        # 训练模型\n",
    "        with tf.Session() as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            saver = tf.train.Saver()\n",
    "            best_f1 = 0.0\n",
    "            for time in range(con.epoch):\n",
    "                print(\"training \" + str(time + 1) + \">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "                model.is_trainning = True\n",
    "                loss_all = []\n",
    "                accuracy_all = []\n",
    "                # 1轮训练所有样本，分batch训练\n",
    "                for texta, textb, tag in tqdm(\n",
    "                        self.get_batches(train_texta_embedding, train_textb_embedding, train_tag)):\n",
    "                    feed_dict = {\n",
    "                        model.text_a: texta,\n",
    "                        model.text_b: textb,\n",
    "                        model.y: tag,\n",
    "                        model.dropout_keep_prob: con.dropout_keep_prob,\n",
    "                        model.a_length: np.array(self.get_length(texta)),\n",
    "                        model.b_length: np.array(self.get_length(textb))\n",
    "                    }\n",
    "                    _, cost, accuracy = sess.run([model.train_op, model.loss, model.accuracy], feed_dict)\n",
    "                    loss_all.append(cost)\n",
    "                    accuracy_all.append(accuracy)\n",
    "\n",
    "                print(\"第\" + str((time + 1)) + \"次迭代的损失为：\" + str(np.mean(np.array(loss_all))) + \";准确率为：\" +\n",
    "                      str(np.mean(np.array(accuracy_all))))\n",
    "\n",
    "                def dev_step():\n",
    "                    \"\"\"\n",
    "                    Evaluates model on a dev set\n",
    "                    \"\"\"\n",
    "                    loss_all = []\n",
    "                    accuracy_all = []\n",
    "                    predictions = []\n",
    "                    for texta, textb, tag in tqdm(\n",
    "                            self.get_batches(dev_texta_embedding, dev_textb_embedding, dev_tag)):\n",
    "                        feed_dict = {\n",
    "                            model.text_a: texta,\n",
    "                            model.text_b: textb,\n",
    "                            model.y: tag,\n",
    "                            model.dropout_keep_prob: 1.0, # 测试\n",
    "                            model.a_length: np.array(self.get_length(texta)),\n",
    "                            model.b_length: np.array(self.get_length(textb))\n",
    "                        }\n",
    "                        dev_cost, dev_accuracy, prediction = sess.run([model.loss, model.accuracy,\n",
    "                                                                       model.prediction], feed_dict)\n",
    "                        loss_all.append(dev_cost)\n",
    "                        accuracy_all.append(dev_accuracy)\n",
    "                        predictions.extend(prediction)\n",
    "                    y_true = [np.nonzero(x)[0][0] for x in dev_tag]\n",
    "                    y_true = y_true[0:len(loss_all)*con.Batch_Size]\n",
    "                    f1 = f1_score(np.array(y_true), np.array(predictions), average='weighted')\n",
    "                    print('分类报告:\\n', metrics.classification_report(np.array(y_true), predictions))\n",
    "                    print(\"验证集：loss {:g}, acc {:g}, f1 {:g}\\n\".format(np.mean(np.array(loss_all)),\n",
    "                                                                      np.mean(np.array(accuracy_all)), f1))\n",
    "                    return f1\n",
    "\n",
    "                model.is_trainning = False\n",
    "                f1 = dev_step()\n",
    "\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    saver.save(sess, \"save_model/ckpt/model.ckpt\")\n",
    "                    print(\"Saved model success\\n\")\n",
    "\n",
    "\n",
    "\n",
    "train = TrainModel()\n",
    "train.trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from save_model/ckpt\\model.ckpt\n",
      "(array([1], dtype=int64), array([[0.40184772, 0.5981523 ]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import tensorflow.contrib.learn as learn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import data_prepare\n",
    "\n",
    "data_pre = Data_Prepare()\n",
    "\n",
    "\n",
    "class Infer(object):\n",
    "    \"\"\"\n",
    "        ues model to predict classification.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab_processor = learn.preprocessing.VocabularyProcessor.restore('save_model/vocab/vocab.pickle')\n",
    "        self.checkpoint_file = tf.train.latest_checkpoint('save_model/ckpt')\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "            self.sess = tf.Session(config=session_conf)\n",
    "            with self.sess.as_default():\n",
    "                # Load the saved meta graph and restore variables\n",
    "                saver = tf.train.import_meta_graph(\"{}.meta\".format(self.checkpoint_file))\n",
    "                saver.restore(self.sess, self.checkpoint_file)\n",
    "\n",
    "                # Get the placeholders from the graph by name\n",
    "                self.text_a = graph.get_operation_by_name(\"esim_model/text_a\").outputs[0]\n",
    "                self.text_b = graph.get_operation_by_name(\"esim_model/text_b\").outputs[0]\n",
    "                self.a_length = graph.get_operation_by_name(\"esim_model/a_length\").outputs[0]\n",
    "                self.b_length = graph.get_operation_by_name(\"esim_model/b_length\").outputs[0]\n",
    "                self.drop_keep_prob = graph.get_operation_by_name(\"esim_model/dropout_keep_prob\").outputs[0]\n",
    "\n",
    "                # Tensors we want to evaluate\n",
    "                self.prediction = graph.get_operation_by_name(\"esim_model/output/prediction\").outputs[0]\n",
    "                self.score = graph.get_operation_by_name(\"esim_model/output/score\").outputs[0]\n",
    "\n",
    "    def infer(self, sentenceA, sentenceB):\n",
    "        # transfer to vector\n",
    "        sentenceA = [data_pre.pre_processing(sentenceA)]\n",
    "        sentenceB = [data_pre.pre_processing(sentenceB)]\n",
    "        vector_A = np.array(list(self.vocab_processor.transform(sentenceA)))\n",
    "        vector_B = np.array(list(self.vocab_processor.transform(sentenceB)))\n",
    "        feed_dict = {\n",
    "            self.text_a: vector_A,\n",
    "            self.text_b: vector_B,\n",
    "            self.drop_keep_prob: 1.0,\n",
    "            self.a_length: np.array([len(sentenceA[0].split(\" \"))]),\n",
    "            self.b_length: np.array([len(sentenceB[0].split(\" \"))])\n",
    "        }\n",
    "        y, s = self.sess.run([self.prediction, self.score], feed_dict)\n",
    "        return y, s\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    infer = Infer()\n",
    "    sentencea = '你点击详情'\n",
    "    sentenceb = '您点击详情'\n",
    "    print(infer.infer(sentencea, sentenceb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 3)\n",
      "(2, 3, 3)\n",
      "[[[ 1  4  7]\n",
      "  [ 2  5  8]\n",
      "  [ 3  6  9]]\n",
      "\n",
      " [[11 44 77]\n",
      "  [22 55 88]\n",
      "  [33 66 99]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "a = np.array([[\n",
    "        [1,2,3],\n",
    "        [4,5,6],\n",
    "        [7,8,9]],\n",
    "        [[11,22,33],\n",
    "        [44,55,66],\n",
    "        [77,88,99]]\n",
    "        ])\n",
    "print(a.shape)\n",
    "result = tf.transpose(a, perm=[0, 2, 1])\n",
    "with tf.Session() as sess:\n",
    "    res = sess.run(result)\n",
    "    print(res.shape)  \n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 3)\n",
      "(3, 3, 2)\n",
      "[[[ 1 11]\n",
      "  [ 4 44]\n",
      "  [ 7 77]]\n",
      "\n",
      " [[ 2 22]\n",
      "  [ 5 55]\n",
      "  [ 8 88]]\n",
      "\n",
      " [[ 3 33]\n",
      "  [ 6 66]\n",
      "  [ 9 99]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "a = np.array([[\n",
    "        [1,2,3],\n",
    "        [4,5,6],\n",
    "        [7,8,9]],\n",
    "        [[11,22,33],\n",
    "        [44,55,66],\n",
    "        [77,88,99]]\n",
    "        ])\n",
    "print(a.shape)\n",
    "result = tf.transpose(a)\n",
    "with tf.Session() as sess:\n",
    "    res = sess.run(result)\n",
    "    print(res.shape)  \n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 3)\n",
      "(2, 3, 3)\n",
      "[[[ 1  2  3]\n",
      "  [ 4  5  6]\n",
      "  [ 7  8  9]]\n",
      "\n",
      " [[11 22 33]\n",
      "  [44 55 66]\n",
      "  [77 88 99]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "a = np.array([[\n",
    "        [1,2,3],\n",
    "        [4,5,6],\n",
    "        [7,8,9]],\n",
    "        [[11,22,33],\n",
    "        [44,55,66],\n",
    "        [77,88,99]]\n",
    "        ])\n",
    "print(a.shape)\n",
    "result = tf.transpose(a)\n",
    "result = tf.transpose(result)\n",
    "with tf.Session() as sess:\n",
    "    res = sess.run(result)\n",
    "    print(res.shape)  \n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
